{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T8YmTG4K5d1"
   },
   "source": [
    "# COMP90086 Workshop 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmhzR5g9K5d3"
   },
   "source": [
    "To perform 3D reconstruction, we need to obtain a depth map. Although there are several ways to obtain a depth map, in this workshop we will only discuss the common RGB cameras. In this case, we need to perform a 3D reconstruction. The key to 3D reconstruction is 3D matching, which is the topic we will discuss today.\n",
    "\n",
    "In this workshop, you will have some practice on the implementation steps of 3D matching.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Implement steps for 3D matching\n",
    "    - Camera Calibration\n",
    "    - Epipolar Geometry\n",
    "    - Depth Map\n",
    "    \n",
    "\n",
    "- Extra bonus\n",
    "    - Pose Estimation\n",
    "    \n",
    "    \n",
    "- Exercises\n",
    "\n",
    "Each section of this content covers the basic techniques that need to be mastered in order to perform 3D matching. This material has been adapted from the [OpenCV documentation](https://docs.opencv.org/4.5.2/d9/db7/tutorial_py_table_of_contents_calib3d.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74PSeIyxL_vp"
   },
   "outputs": [],
   "source": [
    "## For colab user, run\n",
    "\n",
    "#! pip install opencv-contrib-python==4.5.2.52\n",
    "\n",
    "## After the update, You must restart the runtime in order to use newly installed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6he1NotK5d3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Camera Calibration\n",
    "\n",
    "3D matching requires that both images have exactly the same features, in other words, there should be no distortion in either image. However, the reality is that most lenses will distort the photos you take. Therefore we need to correct it. \n",
    "\n",
    "The following diagram shows two common types of radial distortion: barrel distortion and pincushion distortion (Image courtesy: [OpenCV documentation](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html)). \n",
    "\n",
    "<img style=\"float: ;\" src=\"https://docs.opencv.org/2.4/_images/distortion_examples.png\" width=600 height=500>\n",
    "\n",
    "To perform the correction, we need to know the internal parameters of the camera we are using. But in most cases these parameters are unknown.\n",
    "\n",
    "Luckily for you! OpenCV has an algorithm designed for this purpose. The process of calibrating your camera in OpenCV involves taking multiple pictures with the same checkerboard pattern from different angles using your camera (usually at least 10 photos). In this workshop, we will use some examples of chessboard images that come with OpenCV.\n",
    "\n",
    "Once we find these parameters, we can use OpenCV to undistort the images. Come on, let's take the first step towards 3D matching.\n",
    "\n",
    "### Extra bonus: Calibrate for our cameras!\n",
    "\n",
    "- 1. [Get a chessboard pattern](https://docs.opencv.org/2.4/_downloads/pattern.png)\n",
    "- 2. Put the printed chessboard pattern on something flat.\n",
    "<img style=\"float: ;\" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/pattern2.JPG\" width=600 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. Take at least 10 photos of the chessboard pattern with different variation.\n",
    "<img style=\"float: ;\" src=\"https://miro.medium.com/max/960/1*iA71naaNHedLcSUPMN1Zrw.jpeg\" width=600 height=500>\n",
    "(Image courtesy: Utkarsh Sinah from AI shack)\n",
    "- 4. Coding â†“\n",
    "\n",
    "### (i) Set up\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.findChessboardCorners](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a)\n",
    "- [cv2.cornerSubPix](https://docs.opencv.org/4.5.2/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e)\n",
    "- [cv2.drawChessboardCorners](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga6a10b0bb120c4907e5eabbcd22319022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid we are trying to find, \n",
    "# In this case we use 7x6 grid (see the plot show).\n",
    "grid_size = (7,6)\n",
    "\n",
    "# Arrays to store object points and image points\n",
    "obj_points = [] # 3d point in real world space\n",
    "img_points = [] # 2d points in image plane.\n",
    "\n",
    "# Prepare a grid that allows all object points to be stored orderly, \n",
    "# like (0,0,0), (1,0,0),....,(6,5,0)\n",
    "objp = np.zeros((np.prod(grid_size),3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:grid_size[0],0:grid_size[1]].T.reshape(-1,2)\n",
    "\n",
    "# Set the termination criteria for the corner sub-pixel algorithm\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# Here, we care about both accuracy and number of iterations,\n",
    "# stop the algorithm iteration if specified accuracy (0.001),\n",
    "# stop the algorithm after the specified number of iterations (30).\n",
    "\n",
    "img_paths = glob.glob('./chessboard/*')\n",
    "\n",
    "# Deal with multiple images iteratively using glob\n",
    "for name in img_paths:\n",
    "    img = cv2.imread(name)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, grid_size, None)\n",
    "    \n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        obj_points.append(objp)\n",
    "        img_points.append(corners)\n",
    "        \n",
    "        # Refine corner location based on criteria\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)\n",
    "        \n",
    "        # Draw and display the grid on the image\n",
    "        cv2.drawChessboardCorners(img, grid_size, corners2, ret)\n",
    "        \n",
    "        plt.imshow(img, cmap='gray')  \n",
    "        plt.title(name.split('/')[1])\n",
    "        plt.axis('off')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Calibration\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.calibrateCamera](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga687a1ab946686f0d85ae0363b5af1d7b)\n",
    "\n",
    "The calibrateCamera() function returns the intrinsic parameters for this camera (distortion coefficients, focal length, optical center). The camera's focal length $(f_x,f_y)$ and optical centre $(c_x,c_y)$ are expressed in the camera matrix:\n",
    "\n",
    "$$camera matrix = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The calibrateCamera() function also returns the extrinsic parameters for the video sequence (rotation and translation of the camera). Note that for calculations we assume that the chessboard location is fixed and the camera is moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret: a flag\n",
    "# mtx: camera matrix \n",
    "# dist: distortion coefficients\n",
    "# rvecs: rotation vectors\n",
    "# tvecs: translation vectors\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, gray.shape[::-1], None, None)\n",
    "\n",
    "# print the camera matrix\n",
    "print(mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "np.savez('./calib.npz', mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)\n",
    "\n",
    "# Load camera parameters\n",
    "#mtx = np.load('./calib.npz') ['mtx']\n",
    "#dist = np.load('./calib.npz') ['dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Undistortion\n",
    "\n",
    "After undistorted the image, we can see in the result that all the edges are straight.\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.getOptimalNewCameraMatrix](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the test images\n",
    "rootpath='./'\n",
    "img = cv2.imread(os.path.join(rootpath, \"chessboard/left12.jpeg\")) # Try more images!\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Get new camera matrix and undistort image\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))\n",
    "dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "# Crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "\n",
    "# Display the result\n",
    "plt.imshow(dst, cmap='gray')  \n",
    "plt.title('image')\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Camera calibration requires multiple images to disambiguate between intrinsic and extrinsic factors that affect the appearance of the checkerboard in a single image. What happens if you try to calibrate with just one image? (Try a few different images.) Test your calibration by undistorting a test image, and compare the newcameramtx values to the values you obtained from calibrating with the full image set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Epipolar Geometry\n",
    "\n",
    "<center>Epipolar lines/Epiline</center>\n",
    "<img style=\"float: ;\" src=\"https://miro.medium.com/max/1212/1*AtZfO8s0FJGesob9n4ZDLg.png\" width=450 height=500>\n",
    "<center>(Image courtesy: Omar Padierna)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates some basic concepts for capturing the same scene with two cameras (Image courtesy: [Epipolar geometry - Wikipedia](https://en.wikipedia.org/wiki/Epipolar_geometry)). \n",
    "\n",
    "<img style=\"float: ;\" src=\"https://upload.wikimedia.org/wikipedia/commons/1/14/Epipolar_geometry.svg\" width=450 height=500>\n",
    "\n",
    "For more information on the concept of [epipolar geometry](https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT10/node3.html), see the explanation provided by CVonline.\n",
    "\n",
    "### (i) Find the Fundamental Matrix\n",
    "\n",
    "To find the fundamental matrix, we need to have some matches points between two images.\n",
    "\n",
    "Load the left and right images in grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images from a filepath as graycsale.\n",
    "imgL = cv2.imread(os.path.join(rootpath, \"left.jpeg\"),cv2.IMREAD_GRAYSCALE) # left image\n",
    "imgR = cv2.imread(os.path.join(rootpath, \"right.jpeg\"),cv2.IMREAD_GRAYSCALE) # right image\n",
    "\n",
    "# Display the images\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imgL, cmap='gray')  \n",
    "plt.title('Left image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imgR, cmap='gray')  \n",
    "plt.title('Right image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use SIFT descriptors with FLANN based matchers and ratio tests to find matches points between two images for computing the fundamental matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "# your code\n",
    "\n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "# your code\n",
    "kp1, des1 =  # Correspond to the imgL\n",
    "kp2, des2 =  # Correspond to the imgR\n",
    "\n",
    "\n",
    "# FLANN parameters and initialize\n",
    "# your code\n",
    "\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "# your code\n",
    "\n",
    "\n",
    "# Apply ratio test\n",
    "ptsL = []\n",
    "ptsR = []\n",
    "\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.8 *n.distance: # was 0.7\n",
    "        ptsL.append(kp1[m.queryIdx].pt)\n",
    "        ptsR.append(kp2[m.trainIdx].pt)\n",
    "\n",
    "ptsL = np.int32(ptsL)\n",
    "ptsR = np.int32(ptsR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the fundamental matrix based on the best matches.\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.findFundamentalMat](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#gae850fad056e407befb9e2db04dd9e509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, mask = cv2.findFundamentalMat(ptsL,ptsR,cv2.FM_LMEDS)\n",
    "\n",
    "# Select only inlier points\n",
    "ptsL = ptsL[mask.ravel()==1]\n",
    "ptsR = ptsR[mask.ravel()==1]\n",
    "\n",
    "# print the fundamental matrix\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the epilines (epipolar lines) in both the images and draw these lines on the images.\n",
    "Epilines correspond to points in the opposing image.\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- Drawing Functions: &ensp;[cv2.line](https://docs.opencv.org/4.5.2/d6/d6e/group__imgproc__draw.html#ga7078a9fae8c7e7d13d24dac2520ae4a2)&ensp;&ensp;[cv2.circle](https://docs.opencv.org/4.5.2/d6/d6e/group__imgproc__draw.html#gaf10604b069374903dbd0f0488cb43670)\n",
    "-[cv2.computeCorrespondEpilines](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to draw epilines over the images\n",
    "# Epilines corresponding to the points in first image is drawn on second image\n",
    "def drawlines(img1,img2,lines,pts1,pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    r,c = img1.shape\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "    for r,pt1,pt2 in zip(lines, pts1, pts2):\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color, 1)\n",
    "        img1 = cv2.circle(img1, tuple(pt1), 5, color, -1)\n",
    "        img2 = cv2.circle(img2, tuple(pt2), 5, color, -1)\n",
    "    return img1,img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find epilines corresponding to points in right image \n",
    "# and draw its lines on left image\n",
    "linesL = cv2.computeCorrespondEpilines(ptsR.reshape(-1,1,2), 2, F)\n",
    "linesL = linesL.reshape(-1,3)\n",
    "img5, img6 = drawlines(imgL, imgR, linesL, ptsL, ptsR)\n",
    "\n",
    "# Find epilines corresponding to points in left image \n",
    "# and draw its lines on right image\n",
    "linesR = cv2.computeCorrespondEpilines(ptsL.reshape(-1,1,2), 1, F)\n",
    "linesR = linesR.reshape(-1,3)\n",
    "img3, img4 = drawlines(imgR, imgL, linesR, ptsR, ptsL)\n",
    "\n",
    "# Display the results\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img5, cmap='gray')  \n",
    "plt.title('Drawn on the left image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img3, cmap='gray')  \n",
    "plt.title('Drawn on the right image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results display two images with epilines which correspond to points from the opposing image. You may notice that the meeting point (i.e. epipole) of all epilines occurs outside of the view. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Stereo Depth Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We intend to compute the depth of objects detected in a group of stereo images. To do so, we need to generate a greyscale heat map, where lighter greys represent objects close to the lens, and darker greys mean objects further away.\n",
    "\n",
    "OpenCV provides built-in functions for generating a depth map of a set of stereo images. The following code demonstrates this process.\n",
    "\n",
    "Note: \n",
    "- To use this function with your own captured images, you should always perform a camera calibration first and then undistort your images. Camera calibration only needs to be performed once, unless you change cameras. Nevertheless, you should always undistort your images each time you take some new ones.\n",
    "\n",
    "- It is important to make both cameras at the same height (as our eyes). Check out [this blog](https://erget.wordpress.com/2014/02/01/calibrating-a-stereo-camera-with-opencv/) on how to make your own dual camera system for better results. If you are going to use your phone camera to capture images, please move the camera horizontally very carefully to ensure there is no vertical movement (try using a tripod for example). \n",
    "\n",
    "More information on the functions used below, see:\n",
    " - [cv2.StereoBM_create](https://docs.opencv.org/4.5.2/d9/dba/classcv_1_1StereoBM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in images from a filepath as graycsale\n",
    "imgL= cv2.imread(os.path.join(rootpath, \"tsukuba_l.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "imgR = cv2.imread(os.path.join(rootpath, \"tsukuba_r.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Computing stereo correspondence using the block matching algorithm.\n",
    "# Initialize the stereo block matching object \n",
    "''' numDisparities - the disparity search range. For each pixel algorithm will find \n",
    "                     the best disparity from 0 (default minimum disparity) to numDisparities. \n",
    "                     The search range can then be shifted by changing the minimum disparity.\n",
    "    blockSize - the linear size of the blocks compared by the algorithm. \n",
    "                The size should be odd (as the block is centered at the current pixel). \n",
    "                Larger block size implies smoother, though less accurate disparity map. \n",
    "                Smaller block size gives more detailed disparity map, \n",
    "                but there is higher chance for algorithm to find a wrong correspondence. '''\n",
    "num_disp = 32 # try to tune the parameters to get better&smooth result\n",
    "block_size = 21\n",
    "stereoBM = cv2.StereoBM_create(numDisparities=num_disp,\n",
    "                             blockSize=block_size)\n",
    "\n",
    "# Compute the disparity image\n",
    "disparityBM = stereoBM.compute(imgL,imgR)\n",
    "\n",
    "# Scale the pixel values to between 0-255 (Or you can normalized to [0,1])\n",
    "min = disparityBM.min()\n",
    "max = disparityBM.max()\n",
    "disparityBM = ((disparityBM - min) / (max - min)*255).astype(np.uint8)\n",
    "\n",
    "# Computing stereo correspondence using the semi-global block matching algorithm.\n",
    "# This method uses block matching with additional constraints (disparity smoothness)\n",
    "# Initialize the stereo block matching object \n",
    "stereoSGBM = cv2.StereoSGBM_create(numDisparities=num_disp,\n",
    "                             blockSize=block_size)\n",
    "\n",
    "# Compute the disparity image\n",
    "disparitySGBM = stereoSGBM.compute(imgL,imgR)\n",
    "\n",
    "# Scale the pixel values to between 0-255 (Or you can normalized to [0,1])\n",
    "min = disparitySGBM.min()\n",
    "max = disparitySGBM.max()\n",
    "disparitySGBM = ((disparitySGBM - min) / (max - min)*255).astype(np.uint8)\n",
    "\n",
    "# Display the result\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(imgL, cmap='gray')  \n",
    "plt.title('Left image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(imgR, cmap='gray')  \n",
    "plt.title('Right image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(disparityBM,'gray')\n",
    "plt.title('Disparity Image - Block Matching')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(disparitySGBM,'gray')\n",
    "plt.title('Disparity Image - Semi-Global Block Matching')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "What is the effect of changing numDisparities and blockSize? How do the block matching and semi-global block matching results differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Use the cv2.StereoSGBM_create function (the semi-global block matching algorithm) for generating a depth map of a set of stereo images.\n",
    "\n",
    "Note: SGBM works with either grayscale or color images. BM only works with grayscale.\n",
    "\n",
    "More information on the functions and parameters used below, see:\n",
    " - [cv2.StereoSGBM_create](https://docs.opencv.org/4.5.2/d2/d85/classcv_1_1StereoSGBM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images from a filepath and downscale images for faster processing\n",
    "imgL= cv2.pyrDown(cv2.imread(os.path.join(rootpath, 'aloeL.jpeg'))) \n",
    "imgR = cv2.pyrDown(cv2.imread(os.path.join(rootpath, 'aloeR.jpeg')))\n",
    "\n",
    "# Set disparity parameters\n",
    "# your code\n",
    "\n",
    "stereo = cv2.StereoSGBM_create(\n",
    "    # your code\n",
    "    )\n",
    "\n",
    "# Compute the disparity image\n",
    "disp = stereo.compute(imgL,imgR)\n",
    "\n",
    "# Scale the pixel values to between 0-255 for a grayscale image\n",
    "# your code\n",
    "\n",
    "# Convert the colour from BGR to RGB for display\n",
    "color_imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "color_imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the result\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(color_imgL)  \n",
    "plt.title('Left image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(color_imgR)  \n",
    "plt.title('Right image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(disp,'gray')\n",
    "plt.title('Disparity Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Bonus: 3D effects\n",
    "\n",
    "This is an extra bonus for you to create some cool 3D effects on 2D images, using the basic principles behind Augmented Reality algorithms.\n",
    "\n",
    "We have learned how to find the camera matrix, distortion coefficients, etc. Now we can utilize the parameters obtained from the camera calibration section to estimate the positions of objects in the real-world space (in this case, the checkerboard). Once we know the position of an object in space, we can draw some 2D diagrams in the image to simulate a 3D object in the real world. In the example below, we will draw some simple shapes (a 3D coordinate axes or a 3D cube) in such a way that they appear to be attached to the chessboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draws 3D coordinate axes (X, Y, Z axes) on the first corner of chessboard\n",
    "def draw_axes(img, corners, imgPoints):\n",
    "    corner = tuple(corners[0].ravel())\n",
    "    img = cv2.line(img, corner, tuple(imgPoints[0].ravel()), (255, 0, 0), 5)\n",
    "    img = cv2.line(img, corner, tuple(imgPoints[1].ravel()), (0, 255, 0), 5)\n",
    "    img = cv2.line(img, corner, tuple(imgPoints[2].ravel()), (0, 0, 255), 5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draws 3D cube on the first corner of chessboard\n",
    "def draw_cube(img, corners, imgpts):\n",
    "    imgpts = np.int32(imgpts).reshape(-1,2)\n",
    "    # draw ground floor in green\n",
    "    img = cv2.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\n",
    "    # draw pillars in blue color\n",
    "    for i,j in zip(range(4),range(4,8)):\n",
    "        img = cv2.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)\n",
    "    # draw top layer in red color\n",
    "    img = cv2.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved camera calibration parameters\n",
    "with np.load('./calib.npz') as calibPara:\n",
    "    mtx, dist, rvecs, tvecs = [calibPara[i] for i in ('mtx', 'dist', 'rvecs', 'tvecs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function draws 3D coordinate axes(AXIX or 0) or 3D cube (CUBE or 1) based on the input \n",
    "AXIX=0\n",
    "CUBE=1\n",
    "def draw_3d(method):\n",
    "    # Define the rows&cols we are trying to find\n",
    "    rows = 7\n",
    "    cols = 6\n",
    "\n",
    "    # Set the termination criteria for the corner sub-pixel algorithm\n",
    "    criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 30, 0.001)\n",
    "\n",
    "    # Prepare a grid that allows all object points to be stored orderly\n",
    "    objectPoints = np.zeros((rows * cols, 1, 3), np.float32)\n",
    "    objectPoints[:, :, :2] = np.mgrid[0:rows, 0:cols].T.reshape(-1, 1, 2)\n",
    "\n",
    "    # Create the axis points for draw_axis function.\n",
    "    ''' Draw axis of length 3.\n",
    "        X axis is drawn from (0,0,0) to (3,0,0), \n",
    "        Y axis is drawn from (0,0,0) to (0,3,0),\n",
    "        Z axis is drawn from (0,0,0) to (0,0,-3). \n",
    "        Negative denotes it is drawn towards the camera.\n",
    "    '''\n",
    "    axisPoints_axis = np.float32([[3, 0, 0], [0, 3, 0], [0, 0, -3]]).reshape(-1, 3) \n",
    "\n",
    "    # Create the axis points for draw_cube function\n",
    "    axisPoints_cube =  np.float32([[0,0,0], [0,3,0], [3,3,0], [3,0,0],\n",
    "                                   [0,0,-3],[0,3,-3],[3,3,-3],[3,0,-3]]) #the 8 corners of a cube in 3D space\n",
    "\n",
    "    # Loop over the image files\n",
    "    for name in glob.glob('./chessboard/*'):\n",
    "        # Load the image and convert it to gray scale\n",
    "        img = cv2.imread(name)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chess board corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, grid_size, None)\n",
    "\n",
    "        # Make sure the chess board pattern was found in the image\n",
    "        if ret:\n",
    "            # Refine corner location based on criteria\n",
    "            corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "\n",
    "            # Find the rotation and translation vectors\n",
    "            val, rvecs, tvecs, inliers = cv2.solvePnPRansac(objectPoints, corners, mtx, dist)\n",
    "\n",
    "            if method==AXIX:\n",
    "                # Project the 3D axis points to the image plane\n",
    "                axisImgPoints, jac = cv2.projectPoints(axisPoints_axis, rvecs, tvecs, mtx, dist)\n",
    "            \n",
    "                # Draw 3D coordinate axes\n",
    "                img = draw_axes(img, np.int32(corners), np.int32(axisImgPoints))\n",
    "            \n",
    "            elif method==CUBE:\n",
    "                # Project the 3D axis points to the image plane\n",
    "                axisImgPoints, jac = cv2.projectPoints(axisPoints_cube, rvecs, tvecs, mtx, dist)\n",
    "                \n",
    "                # Draw cube\n",
    "                img = draw_cube(img, np.int32(corners), np.int32(axisImgPoints))\n",
    "\n",
    "        # Display the results\n",
    "        plt.imshow(img, cmap='gray')  \n",
    "        plt.title(name.split('/')[1])\n",
    "        plt.axis('off')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_3d(AXIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_3d(CUBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "worksheet07_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
