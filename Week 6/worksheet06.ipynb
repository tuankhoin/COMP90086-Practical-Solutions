{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74f3d74",
   "metadata": {},
   "source": [
    "# COMP90086 Workshop 6\n",
    "\n",
    "***\n",
    "In this worksheet, we'll see how to use pre-trained CNNs on [ImageNet](https://www.image-net.org/) to conduct classification and feature extraction. We will also try some visualisations of the higher-level features.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- Apply Deep CNN for classification\n",
    "\n",
    "- Apply Deep CNN for feature extraction\n",
    "     \n",
    "- Visualizing intermediate activations\n",
    "\n",
    "- Grad-CAM visualization\n",
    "\n",
    "- Bonus: Guided Backprop Visualization\n",
    "\n",
    "*The images used in this tutorial are from ImageNet or sharing copyright-free images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fcac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# you may want to keep logging enabled when doing your own work\n",
    "# disable Tensorflow warnings for this tutorial\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) \n",
    "# disable Keras warnings for this tutorial\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ab6fd",
   "metadata": {},
   "source": [
    "## 1. Apply Deep CNN for classification\n",
    "\n",
    "We will load a deep model, i.e., [MobileNet](https://arxiv.org/abs/1704.04861) with weights pre-trained on ImageNet, to conduct prediction on some example images. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks, which are popularly for mobile and embedded vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba40f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "\n",
    "# Load the model\n",
    "model = MobileNet (weights='imagenet') #load weights pre-trained on large-scale imagenet dataset\n",
    "# Default input shape has to be (224, 224, 3)\n",
    "img_size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34768729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2157711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the input image\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 224x224\n",
    "    img = image.load_img(img_path, target_size=size)\n",
    "    # Convert the image pixels to a numpy array\n",
    "    # if size is (224,224), `array` is a float32 Numpy array of shape (224, 224, 3)\n",
    "    array = image.img_to_array(img)\n",
    "    # Reshape data for the model\n",
    "    # We add a dimension to transform our array into a \"batch\" of size (1, 224, 224, 3)\n",
    "    # Convert the input to 4D input makes it the same as the training process,\n",
    "    # with a placeholder, rows, cols, the number of colour channels as training process\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    # axis: Position in the expanded axes where the new axis (or axes) is placed.\n",
    "    # axis = 0: insert axis at position 0 \n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cf5b3",
   "metadata": {},
   "source": [
    "### Make a prediction\n",
    "\n",
    "***Hint: Try refreshing several times to see different examples***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec66394",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst = ['wallaby.jpeg','mushroom.png', \"maltese.jpeg\", \"cat.jpg\", \"wombat.jpg\", \"koala.jpg\", \"dingo.jpg\", \"quokka.jpg\"]\n",
    "# select a random image name from the image list\n",
    "img_name = np.random.choice(img_lst)\n",
    "\n",
    "# Prepare image for the model\n",
    "k_img = preprocess_input(get_img_array(img_name, size=img_size))\n",
    "# predict the probability across all output classes\n",
    "# model.predict(input) return the same data structure as your input\n",
    "preds = model.predict(k_img)\n",
    "\n",
    "\n",
    "print(img_name.split('.')[0],'prediction:')\n",
    "# convert the probabilities to class labels and retrieve the top 5 highest probability result\n",
    "for pred in decode_predictions(preds, top=5)[0]: \n",
    "    # print as labels: probability\n",
    "    print('%s: %.3f%%' % (pred[1], pred[2]*100))\n",
    "\n",
    "# display the image\n",
    "im = plt.imread(img_name)\n",
    "plt.imshow(im)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864fa62",
   "metadata": {},
   "source": [
    "### Small detour: Getting Help in a Jupyter Notebook\n",
    "After importing a module, you can view help on the imported module by typing the module name followed by a question mark `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394c8df",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Load the MobileNet model with randomly **initialized weights** and conduct prediction on the above images. How do the predictions change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ca719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83422dd",
   "metadata": {},
   "source": [
    "With random weights, the predictions of the network are random and not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeeb6c5",
   "metadata": {},
   "source": [
    "## 2 Use Deep CNN for feature extraction \n",
    "\n",
    "We will load a deep model, i.e., [ResNet50](https://arxiv.org/abs/1512.03385), to conduct prediction on some example images. The ResNet architecture was developed in 2015 and introduced *residual connections* - which replaces the standard parametric transform at each layer:\n",
    "\n",
    "$$ \\mathbf{x}_{l+1} = F_{\\theta_l}(\\mathbf{x}_l) $$\n",
    "\n",
    "with a transformation of the rough form:\n",
    "\n",
    "$$ \\mathbf{x}_{l +1} = \\varphi\\left( \\mathbf{x}_l + F_{\\theta_l}(\\mathbf{x}_l)\\right), \\, \\varphi \\, \\text{some nonlinearity.}$$\n",
    "\n",
    "The intuition here is that it is a more straightforward task for the network to learn the *residual* - the difference between the input and output of the layer. If little additional processing is required, then the learned transformation $F_{\\theta_l}$ is free to be close to the identity.\n",
    "\n",
    "\n",
    "### Load Deep CNN\n",
    "Load the ResNet50 with weights pre-trained on ImageNet.\n",
    "\n",
    "Note that we only go up to the last convolutional layer --we don't include fully-connected layers by setting `include_top=False`. The reason is that adding the fully connected layers forces you to use a fixed input size for the model (224x224, the original ImageNet format). By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Build a ResNet50 model loaded with pre-trained ImageNet weights\n",
    "# we can set the input size to be the size of the input image (not necessarily 224x224)\n",
    "base_model = ResNet50(weights='imagenet', input_shape=[128,128,3],include_top=False) \n",
    "img_size = (128,128) # default input shape has to be (224, 224, 3)\n",
    "\n",
    "# Print the names of layers\n",
    "for idx in range(len(base_model.layers)):\n",
    "    print(base_model.get_layer(index = idx).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958823aa",
   "metadata": {},
   "source": [
    "### Exercise 2: Use Pre-trained deep CNN for feature extraction\n",
    "\n",
    "Use the above pre-trained model to extract feature from the image. Use the output of the \"conv3_block4_out\" layer as the feature output.\n",
    "\n",
    "***Hint: create a model with `base_model.input` as the intput and the output of the `\"conv3_block4_out\"` as the output, then processs the input, and conduct prediction using `model.predict`.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fde0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code\n",
    "\n",
    "# Extract features from an arbitrary intermediate layer with VGG19\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225b84c",
   "metadata": {},
   "source": [
    "## 3 Visualizing intermediate activations\n",
    "\n",
    "We will use the [VGG19 model](https://arxiv.org/abs/1409.1556) to extract features from raw images. The VGG19 model was a seminal architecture developed in 2014 that represented one of the earliest extremely deep model architectures, achieving state-of-the-art performance on standard vision benchmarks at the cost of expensive computation.\n",
    "\n",
    "Today, it is regularly used as a standard vision 'backbone' - serving as a useful generic visual feature extractor that serves as a platform for task-specific downstream models to build on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "\n",
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "model = VGG19(weights='imagenet') \n",
    "\n",
    "# default input shape has to be (224, 224, 3)\n",
    "img_size = (224,224) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2004b",
   "metadata": {},
   "source": [
    "Loading an input image — a picture of a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the image\n",
    "img_tensor = get_img_array('cat.jpg', size=img_size) / 255.\n",
    "  \n",
    "# Print image tensor shape\n",
    "print(img_tensor.shape)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1a0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize Model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a11508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the ouput of the top 12 layers (excluding the input layer)\n",
    "layer_outputs = [layer.output for layer in model.layers[1:13]]  \n",
    "# Creates a model that will return these outputs, given the model input\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "# Returns a list of five Numpy arrays: one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)\n",
    "# Store the name of the layers\n",
    "layer_names = []\n",
    "for layer in model.layers[1:13]:\n",
    "    layer_names.append(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01c4e4",
   "metadata": {},
   "source": [
    "Visualization of the activation maps from first convolution layer. Initial layers identify low-level features. Different filters activate different parts of the image, like some are detecting edges, some are detecting background, while others are detecting just the outer boundary of the cat  and so on. At that stage, the activations retain almost all of the information present in the initial picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Activations of the first convolution layer for the cat image input\n",
    "first_layer_activation = activations[0]\n",
    "\n",
    "print(layer_names[0])\n",
    "# Shape of first layer activation\n",
    "# It’s a 224 x 224 feature map with 64 channels\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "# Visualizing the first 16 channels in the 64 channels of the first convolution layer\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for img in range(16):\n",
    "    ax = fig.add_subplot(4, 4, img+1)\n",
    "    ax = plt.imshow(activations[0][0, :, :, img], cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8026180",
   "metadata": {},
   "source": [
    "We are now going to visualise each channel of these intermediate activation. Each channel encodes relatively independent features, so we will visualise these feature maps by plotting each channel independently as a 2D image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_per_row = 8    \n",
    "\n",
    "# Displays the activations\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # Number of channels in the layer\n",
    "    n_channel = layer_activation.shape[-1]\n",
    "    # The feature map with shape (1, size, size, n_channel).\n",
    "    size = layer_activation.shape[1]\n",
    "    # Tiles the activation channels in this matrix\n",
    "    n_col = n_channel // img_per_row\n",
    "    display_grid = np.zeros((size * n_col, img_per_row * size))\n",
    "    # Tiles each filter into a big horizontal grid\n",
    "    for col in range(n_col):\n",
    "        for row in range(img_per_row):\n",
    "            channel_image = layer_activation[0,:, :,col * img_per_row + row]\n",
    "            # Post-processes the feature to make it visually palatable\n",
    "            channel_image = (channel_image-channel_image.mean())//channel_image.std()  \n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            # Displays the grid\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "            row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8b69e",
   "metadata": {},
   "source": [
    "Looking at the activations of these convolutional layers, we can see that the layers close to the input layer learn very basic features, such as the edges and textures of the image. But as the layers go deeper, the activations become increasingly abstract and less visually interpretable. The network starts to learn more abstract features, leaving behind the general features of the image, which helps it to classify the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83048541",
   "metadata": {},
   "source": [
    "# 4 Grad-CAM visualization\n",
    "\n",
    "In this section, we will display how to obtain a class activation heatmap for an image classification model. We do that using a method called [Grad-CAM](https://arxiv.org/abs/1610.02391). Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. \n",
    "\n",
    "For visualization purposes, we will continue to use the VGG19 model loaded in the previous section. Reference source for the content of this section: Keras.\n",
    "\n",
    "### The Grad-CAM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05013b",
   "metadata": {},
   "source": [
    "Check the model by the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73898b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae151489",
   "metadata": {},
   "source": [
    "Next, we are going to make a prediction about our sample image. To get the values for `last_conv_layer_name` use `model.summary()` to see the names of all layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a419c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"block5_conv4\"\n",
    "\n",
    "img_name = \"maltese.jpeg\"\n",
    "\n",
    "# Display the image\n",
    "im = plt.imread(img_name)\n",
    "plt.imshow(im)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Prepare image for the model\n",
    "img_array = preprocess_input(get_img_array(img_name, size=img_size))\n",
    "\n",
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "for pred in decode_predictions(preds, top=1)[0]: \n",
    "    # print as labels: probability\n",
    "    print('%s: %.3f%%' % (pred[1], pred[2]))\n",
    "print(\"Class ID:\" ,np.argsort(preds)[0, ::-1][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f0a30",
   "metadata": {},
   "source": [
    "Indeed there is a Maltese dog in the image. The `class ID` (index into the output layer) 153 stands for Maltese dog in [ImageNet with 1000 classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a#file-imagenet1000_clsidx_to_labels-txt-L154). But how does the network know this? Let's classify our images and see where the network \"looks\" when it does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.title('Heatmap')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a superimposed visualization\n",
    "def display_gradcam(img_name, heatmap, alpha=0.4):\n",
    "\n",
    "    # Load the original image\n",
    "    img = image.load_img(img_name)\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Display superimpose result\n",
    "    plt.matshow(superimposed_img)\n",
    "    plt.title('Superimpose')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gradcam(img_name, heatmap, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ac81a",
   "metadata": {},
   "source": [
    "### Exercise 3: Choosing the target class (target prediction) for Grad-CAM\n",
    "\n",
    "We will see how the grad cam explains the model's outputs for a multi-label image. Let's try an image with different fruits together, and see how the Grad-CAM behaves. Please generate Grad-CAM heatmaps for the top2 predicted  categories separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"fruits.jpg\"\n",
    "\n",
    "# Display the image\n",
    "im = plt.imread(img_name)\n",
    "plt.imshow(im)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Prepare image for the model\n",
    "img_array = preprocess_input(get_img_array(img_name, size=img_size))\n",
    "\n",
    "# Print what the top2 predicted categories are\n",
    "preds = model.predict(img_array)\n",
    "for pred in decode_predictions(preds, top=2)[0]: \n",
    "    # print as labels: probability\n",
    "    print('%s: %.3f%%' % (pred[1], pred[2]))\n",
    "print(\"Class ID:\" ,np.argsort(preds)[0, ::-1][:2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000ffe2",
   "metadata": {},
   "source": [
    "First, we generate class activation heatmap for \"Granny_Smith,\" the class index is 948."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c92a3a",
   "metadata": {},
   "source": [
    "Then we generate class activation heatmap for \"banana,\" the class index is 954."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29145937",
   "metadata": {},
   "source": [
    "# Bonus: Guided Backprop Visualization\n",
    "\n",
    "For visualization purposes, we will continue to use the VGG19 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"wombat.jpg\"\n",
    "img = image.load_img(img_name, target_size=img_size)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Prepare image for the model\n",
    "preprocessed_input = preprocess_input(get_img_array(img_name, size=img_size))\n",
    "\n",
    "# Print what the top2 predicted categories are\n",
    "preds = model.predict(preprocessed_input)\n",
    "for pred in decode_predictions(preds, top=2)[0]: \n",
    "    # print as labels: probability\n",
    "    print('%s: %.3f%%' % (pred[1], pred[2]))\n",
    "print(\"Class ID:\" ,np.argsort(preds)[0, ::-1][:2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d702d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15790c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the last conv layer for VGG19\n",
    "Layer = 'block5_conv4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08d9bb",
   "metadata": {},
   "source": [
    "We will creat a model until last convolution layer from the VGG19 model. When we use the fully connected layer in the deep learning CNN model, we lose the spatial information which is retained by convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = Model(\n",
    "    inputs = [model.inputs],    \n",
    "    outputs = [model.get_layer(Layer).output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858055cc",
   "metadata": {},
   "source": [
    "Now we define our own custom ReLU function for the backward pass. We use the `@tf.custom_gradient` decorator (see [here](https://www.tensorflow.org/api_docs/python/tf/custom_gradient)) to implement our own new “GuidedRelu” function for the backward pass. It allows the fine-grained control over the gradients for backpropagating non-negative gradients to have a more efficient or numerically stable gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def GuidedRelu(x):\n",
    "    def grad(dy):\n",
    "        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n",
    "    return tf.nn.relu(x), grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178e200",
   "metadata": {},
   "source": [
    "Afterwards, we substitute all old, original ReLU functions with our own “GuidedRelu” function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ec950",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = [layer for layer in gb_model.layers[1:] if hasattr(layer,'activation')]\n",
    "for layer in layer_dict:\n",
    "    if layer.activation == tf.keras.activations.relu:\n",
    "        layer.activation = GuidedRelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8395c",
   "metadata": {},
   "source": [
    "With `tf.GradientTape()` (see [here](https://www.tensorflow.org/api_docs/python/tf/GradientTape)) we get the saliency map. We use the tf.GradientTape() to record the processed input image during the forward pass and calculate the gradients for the backward pass. Basically it is used to capture the gradients of the final (last) convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tp:\n",
    "    inputs = tf.cast(preprocessed_input, tf.float32)\n",
    "    tp.watch(inputs)\n",
    "    outputs = gb_model(inputs)[0]\n",
    "grads = tp.gradient(outputs,inputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e9195",
   "metadata": {},
   "source": [
    "Finally, visualizing the guided backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_prop = grads\n",
    "guided_back_viz = np.dstack((\n",
    "            gb_prop[:, :, 0],\n",
    "            gb_prop[:, :, 1],\n",
    "            gb_prop[:, :, 2],\n",
    "        ))       \n",
    "guided_back_viz -= np.min(guided_back_viz)\n",
    "guided_back_viz /= guided_back_viz.max()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    " \n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(guided_back_viz)\n",
    "plt.title(\"Guided Backprop\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1834946",
   "metadata": {},
   "source": [
    "We can clearly visualise what the network focuses on - the most relevant image features are located around/inside the wombat's head. This is also in line with our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43802005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
