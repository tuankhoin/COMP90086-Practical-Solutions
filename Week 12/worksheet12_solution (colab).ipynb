{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foupFiTGppWa"
      },
      "source": [
        "# COMP90086 Week 12: Object Detection\n",
        "(To be run on Colab Notebook)\n",
        "\n",
        "In this workshop, we will discuss object detection. We will use the [Tensorflow 2 Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) with pre-trained models from the [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) to detect objects in images. We will then visualise and evaluate the results with the open-source tool [FiftyOne](https://voxel51.com/docs/fiftyone/).\n",
        "\n",
        "\n",
        "Table of Contents\n",
        "\n",
        "- 1 Installation\n",
        "    \n",
        "- 2 Object Detection From Pre-trained Model\n",
        "    - (1) Download the model\n",
        "    - (2) Load the model\n",
        "    - (3) Download the test dataset\n",
        "    - (4) Make predictions\n",
        "    \n",
        "- 3 Evaluation\n",
        "    - (1) Definitions of different metrics\n",
        "    - (2) Running evaluation\n",
        "    - (3) Aggregate results\n",
        "    - (4) Evaluating model mAP\n",
        "    - (5) Sample-level analysis\n",
        "    \n",
        "- 4 Exercise\n",
        "\n",
        "\n",
        "> **Important**: If you're running on a local machine, be sure to refer to the file `Workshop12(Local_version)` and follow the [installation instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html). This notebook includes only what's necessary to run in Colab.\n",
        "\n",
        "\n",
        "This tutorial is adapted from the TensorFlow 2 Object Detection API tutorial and the FiftyOne Docs:\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"http://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/?badge=latest\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"/>\n",
        "    View on Object Detection API Document</a>\n",
        "  </td>   \n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://voxel51.com/docs/fiftyone/\">   \n",
        "    <img src=\"https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png\" width=117/>\n",
        "    View on FiftyOne Docs</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Feedback & Tips\n",
        "Overall, everyone seems to do well. Once you get it working properly, you should get most of the scores already:\n",
        "\n",
        "### Assignment 3: Common reason of losing marks:\n",
        "- 'I choose threshold of 2 pixels because the spec said so'\n",
        "- 'Image didn't performed well because SIFT didn't perform well' - Why so?\n",
        "- Code commenting: We need to read your code to understand what you did. If you don't guide us, we have to spend more time doing exploring, and you lose marks.\n",
        "- 'I give up midway' - Code error can be forgiving. We aim to see if you understand the methods. If you can still write code to implement it, we would still give you most of the points (at the worst, you just can't review Task 3 properly, which takes only a small portion).\n",
        "\n",
        "### Project report tips:\n",
        "- It's an art to balance between detailed and conciseness: Include as much details of yours as possible, but remember to keep it short but clear!\n",
        "- We always look for reasoning: If I mark your report and I have to \"hmmmm? Why so?\", then you lose marks.\n",
        "- Visualisation is a good addition to presentation and reasoning.\n",
        "- There is no right, only good arguments (definitely not the Karen style ones).\n",
        "- Remember to make it readable:\n",
        "  - If I don't understand, I can assume you did not do it.\n",
        "  - Best to have a peer reading your report, and see if they understand what you did.\n"
      ],
      "metadata": {
        "id": "MzbvmgC3LTyZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHJuYnZFqDtN"
      },
      "source": [
        "## 1 Installation\n",
        "\n",
        "This may take up about 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BLijLdgTrQQ"
      },
      "source": [
        "!pip install tf_slim\n",
        "!pip install pycocotools\n",
        "!pip install fiftyone\n",
        "#!pip install googledrivedownloader\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pathlib\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")     # Suppress warnings\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')     # Suppress TensorFlow logging\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from PIL import Image\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "    while \"models\" in pathlib.Path.cwd().parts:\n",
        "        os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "    !git clone --depth 1 https://github.com/tensorflow/models\n",
        "\n",
        "os.chdir('models/research/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbgOfM0wqmPh"
      },
      "source": [
        "Enable GPU dynamic memory allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLe39u3FTt27"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q48jZLjrCHf"
      },
      "source": [
        "## 2 Object Detection From Pre-trained Model\n",
        "\n",
        "This demo will take you through the steps of running an \"out-of-the-box\" TensorFlow 2 compatible\n",
        "detection model on a collection of images. We will utilise the [TensorFlow 2 Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). The TensorFlow Object Detection API is an open source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models. \n",
        "\n",
        "### (1) Download the model\n",
        "\n",
        "We will use the pre-trained models from the Model Zoo to detect objects in images. \n",
        "\n",
        "The [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) is a collection of detection models pre-trained on the COCO 2017 dataset. The [COCO 2017 dataset](https://cocodataset.org/#home) is a large-scale dataset of labelled images. It contains 80 common object categories. The models can perform object detection on these object categories. You can explore the COCO dataset by using the the [COCO dataset explorer page](https://cocodataset.org/#explore).\n",
        "\n",
        "You might find that there are plenty of models available in the Model Zoo. The architecture of all these models can be identified by name. In this workshop, we suggest you to try some of the following models. You could compare and contrast the results of the Faster R-CNN, SSD, and CenterNet. You could also try swapping the \"backbone\" of a network (e.g., ResNet vs. MobileNet) to see if it changes the results.\n",
        "\n",
        "| Name | MODEL_NAME   | MODEL_DATE | Speed (ms) | COCO mAP |\n",
        "|------|--------------|------------|------------|----------|\n",
        "|[CenterNet Resnet50 V2 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v2_512x512_coco17_tpu-8.tar.gz)|`centernet_resnet50_v2_512x512_coco17_tpu-8`|`20200711`|27|29.5|\n",
        "|[CenterNet Resnet50 V1 FPN 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v1_fpn_512x512_coco17_tpu-8.tar.gz)|`centernet_resnet50_v1_fpn_512x512_coco17_tpu-8`|`20200711`|27|31.2|\n",
        "|[CenterNet HourGlass104 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz)|`centernet_hg104_512x512_coco17_tpu-8`|`20200713`|70|41.9|\n",
        "|[SSD ResNet50 V1 FPN 640x640 (RetinaNet50)](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz)|`ssd_resnet50_v1_fpn_640x640_coco17_tpu-8`|`20200711`|46|34.3|\n",
        "|[SSD ResNet101 V1 FPN 640x640 (RetinaNet101)](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz)|`ssd_resnet101_v1_fpn_640x640_coco17_tpu-8`|`20200711`|57|35.6|\n",
        "|[SSD MobileNet V1 FPN 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz)|`ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`|`20200711`|48|29.1|\n",
        "|[Faster R-CNN ResNet50 V1 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz)|`faster_rcnn_resnet50_v1_640x640_coco17_tpu-8`|`20200711`|53|29.3|\n",
        "|[Faster R-CNN ResNet50 V1 800x1333](http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8.tar.gz)|`faster_rcnn_resnet50_v1_800x1333_coco17_gpu-8`|`20200711`|65|31.6|\n",
        "|[Faster R-CNN ResNet101 V1 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz)  | `faster_rcnn_resnet101_v1_640x640_coco17_tpu-8` |  `20200711` | 55|   31.8 |\n",
        "|[Faster R-CNN Inception ResNet V2 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8.tar.gz)  | `faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8` |  `20200711` | 206|   37.7 |\n",
        "\n",
        "More models can be found in the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        "\n",
        "#### Metrics in the table\n",
        "    - Speed (ms) is the expected response time to get a response when querying the model, the lower the better.  \n",
        "    - COCO mAP is the mean average precision score against the COCO dataset of images, the higher the better.\n",
        "\n",
        "The code snippet shown below is used to download the pre-trained object detection model we shall\n",
        "use to perform inference. The particular pre-trained model we will use is the `CenterNet MobileNetV2 FPN 512x512`.  To use a different model you will need to:\n",
        "\n",
        "1. Copy the `MODEL_NAME` part of the table and use it to replace the value of the `MODEL_NAME` variable in the code shown below;\n",
        "2. Copy the `MODEL_DATE` part of the table and use it to replace the value of the `MODEL_DATE` variable in the code shown below.\n",
        "\n",
        "For example, if we want to switch to the `CenterNet Resnet50 V1 FPN 512x512` model, then the following block should use code:\n",
        "```\n",
        "MODEL_DATE = '20200711'\n",
        "MODEL_NAME = 'centernet_resnet50_v1_fpn_512x512_coco17_tpu-8'\n",
        "```\n",
        "\n",
        "More information on the functions used below, see:\n",
        "- [`tf.keras.utils.get_file( )`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) Downloads a file from a URL if it not already in the cache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3RkwUWTrgix",
        "outputId": "2a1f2937-6cfb-466d-b03f-2ecbcad8e08d"
      },
      "source": [
        "# Download and extract model\n",
        "def download_model(model_name, model_date):\n",
        "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
        "    model_file = model_name + '.tar.gz'\n",
        "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
        "                                        origin=base_url + model_date + '/' + model_file,\n",
        "                                        untar=True, #Whether the file should be decompressed\n",
        "                                        cache_dir=\"../../\") #Location to store cached files\n",
        "    return str(model_dir)\n",
        "MODEL_DATE = '20200711'\n",
        "MODEL_NAME = 'ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8'\n",
        "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
            "90456064/90453990 [==============================] - 1s 0us/step\n",
            "90464256/90453990 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhShIAYir8nb"
      },
      "source": [
        "### (2) Load the model\n",
        "Next we load the downloaded model. In this example we will be using\n",
        "the [Saved Model Format](https://www.tensorflow.org/guide/saved_model) to load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slf1lKoVrglE",
        "outputId": "88ef8921-7492-4c8e-9c33-777e5a4ad864"
      },
      "source": [
        "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
        "\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "\n",
        "# Load saved model and build the detection function\n",
        "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...Done! Took 18.902263879776 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o3KQ6CHsFae"
      },
      "source": [
        "### (3) Download the test dataset\n",
        "\n",
        "To speed up the experiment, the test set we used in the workshop contained only fifty images. These images and their ground truths are taken from the [COCO 2017 Validation dataset](https://cocodataset.org/#download) (Note: The official COCO test set annotations are unavailable to the public)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJowaH9xuPOh",
        "outputId": "d398c4ff-ad79-470c-cf6d-620f47b9ed20"
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id='1PDUnoCwm2LrYRkDbhjmUEJaPK0EH-LNU',\n",
        "                                    dest_path='../../datasets.zip',\n",
        "                                    unzip=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1PDUnoCwm2LrYRkDbhjmUEJaPK0EH-LNU into ../../datasets.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m042NTagyAOT",
        "outputId": "9c005e14-5388-4c42-edf4-7dd11c94dd39"
      },
      "source": [
        "# Load COCO formatted dataset ground_truth\n",
        "coco_dataset = fo.Dataset.from_dir(\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    data_path='../../datasets/images/',\n",
        "    labels_path='../../datasets/coco.json',\n",
        "    include_id=True,\n",
        "    label_field=\"ground_truth\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████████| 50/50 [383.4ms elapsed, 0s remaining, 132.5 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sovxL4yUsI1X"
      },
      "source": [
        "Let’s inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2an5LnFzrgpo",
        "outputId": "a3492f44-308e-4c53-ce4d-aaf8f88db89b"
      },
      "source": [
        "# Print some information about the dataset\n",
        "print(coco_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:        2021.10.16.06.56.52\n",
            "Media type:  image\n",
            "Num samples: 50\n",
            "Persistent:  False\n",
            "Tags:        []\n",
            "Sample fields:\n",
            "    id:                      fiftyone.core.fields.ObjectIdField\n",
            "    filepath:                fiftyone.core.fields.StringField\n",
            "    tags:                    fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
            "    metadata:                fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
            "    ground_truth_detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
            "    ground_truth_coco_id:    fiftyone.core.fields.IntField\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyyOasn8rgr-",
        "outputId": "560cc409-7765-4860-ee42-91aa057ff57a"
      },
      "source": [
        "# Print a ground truth detection\n",
        "sample = coco_dataset.first()\n",
        "print(sample.ground_truth_detections.detections[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Detection: {\n",
            "    'id': '616a77b536fd785cd314ebf2',\n",
            "    'attributes': BaseDict({}),\n",
            "    'tags': BaseList([]),\n",
            "    'label': 'person',\n",
            "    'bounding_box': BaseList([\n",
            "        0.30445833333333333,\n",
            "        0.46070312500000005,\n",
            "        0.22441666666666665,\n",
            "        0.293421875,\n",
            "    ]),\n",
            "    'mask': None,\n",
            "    'confidence': None,\n",
            "    'index': None,\n",
            "    'supercategory': 'person',\n",
            "    'iscrowd': 0,\n",
            "}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK-52Oa3rguA",
        "outputId": "2efa02f7-160a-4728-fde0-1fdb7d435e0a"
      },
      "source": [
        "# Export labels in COCO format\n",
        "with open(\"../../datasets/coco.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Explore labels    \n",
        "data['categories']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 1, 'name': 'person', 'supercategory': 'person'},\n",
              " {'id': 2, 'name': 'bicycle', 'supercategory': 'vehicle'},\n",
              " {'id': 3, 'name': 'car', 'supercategory': 'vehicle'},\n",
              " {'id': 4, 'name': 'motorcycle', 'supercategory': 'vehicle'},\n",
              " {'id': 5, 'name': 'airplane', 'supercategory': 'vehicle'},\n",
              " {'id': 6, 'name': 'bus', 'supercategory': 'vehicle'},\n",
              " {'id': 7, 'name': 'train', 'supercategory': 'vehicle'},\n",
              " {'id': 8, 'name': 'truck', 'supercategory': 'vehicle'},\n",
              " {'id': 9, 'name': 'boat', 'supercategory': 'vehicle'},\n",
              " {'id': 10, 'name': 'traffic light', 'supercategory': 'outdoor'},\n",
              " {'id': 11, 'name': 'fire hydrant', 'supercategory': 'outdoor'},\n",
              " {'id': 13, 'name': 'stop sign', 'supercategory': 'outdoor'},\n",
              " {'id': 14, 'name': 'parking meter', 'supercategory': 'outdoor'},\n",
              " {'id': 15, 'name': 'bench', 'supercategory': 'outdoor'},\n",
              " {'id': 16, 'name': 'bird', 'supercategory': 'animal'},\n",
              " {'id': 17, 'name': 'cat', 'supercategory': 'animal'},\n",
              " {'id': 18, 'name': 'dog', 'supercategory': 'animal'},\n",
              " {'id': 19, 'name': 'horse', 'supercategory': 'animal'},\n",
              " {'id': 20, 'name': 'sheep', 'supercategory': 'animal'},\n",
              " {'id': 21, 'name': 'cow', 'supercategory': 'animal'},\n",
              " {'id': 22, 'name': 'elephant', 'supercategory': 'animal'},\n",
              " {'id': 23, 'name': 'bear', 'supercategory': 'animal'},\n",
              " {'id': 24, 'name': 'zebra', 'supercategory': 'animal'},\n",
              " {'id': 25, 'name': 'giraffe', 'supercategory': 'animal'},\n",
              " {'id': 27, 'name': 'backpack', 'supercategory': 'accessory'},\n",
              " {'id': 28, 'name': 'umbrella', 'supercategory': 'accessory'},\n",
              " {'id': 31, 'name': 'handbag', 'supercategory': 'accessory'},\n",
              " {'id': 32, 'name': 'tie', 'supercategory': 'accessory'},\n",
              " {'id': 33, 'name': 'suitcase', 'supercategory': 'accessory'},\n",
              " {'id': 34, 'name': 'frisbee', 'supercategory': 'sports'},\n",
              " {'id': 35, 'name': 'skis', 'supercategory': 'sports'},\n",
              " {'id': 36, 'name': 'snowboard', 'supercategory': 'sports'},\n",
              " {'id': 37, 'name': 'sports ball', 'supercategory': 'sports'},\n",
              " {'id': 38, 'name': 'kite', 'supercategory': 'sports'},\n",
              " {'id': 39, 'name': 'baseball bat', 'supercategory': 'sports'},\n",
              " {'id': 40, 'name': 'baseball glove', 'supercategory': 'sports'},\n",
              " {'id': 41, 'name': 'skateboard', 'supercategory': 'sports'},\n",
              " {'id': 42, 'name': 'surfboard', 'supercategory': 'sports'},\n",
              " {'id': 43, 'name': 'tennis racket', 'supercategory': 'sports'},\n",
              " {'id': 44, 'name': 'bottle', 'supercategory': 'kitchen'},\n",
              " {'id': 46, 'name': 'wine glass', 'supercategory': 'kitchen'},\n",
              " {'id': 47, 'name': 'cup', 'supercategory': 'kitchen'},\n",
              " {'id': 48, 'name': 'fork', 'supercategory': 'kitchen'},\n",
              " {'id': 49, 'name': 'knife', 'supercategory': 'kitchen'},\n",
              " {'id': 50, 'name': 'spoon', 'supercategory': 'kitchen'},\n",
              " {'id': 51, 'name': 'bowl', 'supercategory': 'kitchen'},\n",
              " {'id': 52, 'name': 'banana', 'supercategory': 'food'},\n",
              " {'id': 53, 'name': 'apple', 'supercategory': 'food'},\n",
              " {'id': 54, 'name': 'sandwich', 'supercategory': 'food'},\n",
              " {'id': 55, 'name': 'orange', 'supercategory': 'food'},\n",
              " {'id': 56, 'name': 'broccoli', 'supercategory': 'food'},\n",
              " {'id': 57, 'name': 'carrot', 'supercategory': 'food'},\n",
              " {'id': 58, 'name': 'hot dog', 'supercategory': 'food'},\n",
              " {'id': 59, 'name': 'pizza', 'supercategory': 'food'},\n",
              " {'id': 60, 'name': 'donut', 'supercategory': 'food'},\n",
              " {'id': 61, 'name': 'cake', 'supercategory': 'food'},\n",
              " {'id': 62, 'name': 'chair', 'supercategory': 'furniture'},\n",
              " {'id': 63, 'name': 'couch', 'supercategory': 'furniture'},\n",
              " {'id': 64, 'name': 'potted plant', 'supercategory': 'furniture'},\n",
              " {'id': 65, 'name': 'bed', 'supercategory': 'furniture'},\n",
              " {'id': 67, 'name': 'dining table', 'supercategory': 'furniture'},\n",
              " {'id': 70, 'name': 'toilet', 'supercategory': 'furniture'},\n",
              " {'id': 72, 'name': 'tv', 'supercategory': 'electronic'},\n",
              " {'id': 73, 'name': 'laptop', 'supercategory': 'electronic'},\n",
              " {'id': 74, 'name': 'mouse', 'supercategory': 'electronic'},\n",
              " {'id': 75, 'name': 'remote', 'supercategory': 'electronic'},\n",
              " {'id': 76, 'name': 'keyboard', 'supercategory': 'electronic'},\n",
              " {'id': 77, 'name': 'cell phone', 'supercategory': 'electronic'},\n",
              " {'id': 78, 'name': 'microwave', 'supercategory': 'appliance'},\n",
              " {'id': 79, 'name': 'oven', 'supercategory': 'appliance'},\n",
              " {'id': 80, 'name': 'toaster', 'supercategory': 'appliance'},\n",
              " {'id': 81, 'name': 'sink', 'supercategory': 'appliance'},\n",
              " {'id': 82, 'name': 'refrigerator', 'supercategory': 'appliance'},\n",
              " {'id': 84, 'name': 'book', 'supercategory': 'indoor'},\n",
              " {'id': 85, 'name': 'clock', 'supercategory': 'indoor'},\n",
              " {'id': 86, 'name': 'vase', 'supercategory': 'indoor'},\n",
              " {'id': 87, 'name': 'scissors', 'supercategory': 'indoor'},\n",
              " {'id': 88, 'name': 'teddy bear', 'supercategory': 'indoor'},\n",
              " {'id': 89, 'name': 'hair drier', 'supercategory': 'indoor'},\n",
              " {'id': 90, 'name': 'toothbrush', 'supercategory': 'indoor'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aom0yAoXyOxj"
      },
      "source": [
        "#### Visualizing and exploring your dataset with FiftyOne App\n",
        "\n",
        "[FiftyOne](https://voxel51.com/docs/fiftyone/index.html#) is a open-source tool for building datasets and computer vision models. It enables you to visualize datasets and interpret models faster and more effectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "https://localhost:5151/polling?sessionId=93221fb8-5725-4741-9122-abc3109a85a6": {
              "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
              "ok": true,
              "headers": [
                [
                  "access-control-allow-headers",
                  "x-requested-with"
                ],
                [
                  "content-type",
                  "text/html; charset=UTF-8"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SxLdOdf8yKmN",
        "outputId": "ee233ae9-f4f9-47d8-d876-ba39a188d426"
      },
      "source": [
        "session = fo.launch_app(coco_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "#focontainer-797ff264-2746-4e5c-bf6a-40ae1db1a559 {\n",
              "  position: relative;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-797ff264-2746-4e5c-bf6a-40ae1db1a559 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-797ff264-2746-4e5c-bf6a-40ae1db1a559:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-797ff264-2746-4e5c-bf6a-40ae1db1a559 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-797ff264-2746-4e5c-bf6a-40ae1db1a559\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-797ff264-2746-4e5c-bf6a-40ae1db1a559\">\n",
              "      <button id=\"foactivate-797ff264-2746-4e5c-bf6a-40ae1db1a559\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq5MkYezzNSY"
      },
      "source": [
        "#### (Optional) Split your own sub-dataset from the COCO\n",
        "\n",
        "You could also split your own sub-dataset from the COCO 2017 validation dataset.\n",
        "\n",
        "Below we will load 50 random samples from the COCO 2017 validation split that contain cats and dogs. Images that contain all `classes` will be prioritized first, followed by images that contain at least one of the required `classes`. If there are not enough images matching `classes` in the split to meet `max_samples`, only the available images will be loaded. By default, only detections are loaded. By setting `shuffle=True`, it will randomly shuffle the order in which samples are chosen for partial downloads.\n",
        "\n",
        "> **Important**: If you do not intend to split your own dataset, **do not execute the one block of code below**. Once the following one block is executed, FiftyOne will perform a download, which can be very large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1qjGsRsyKoe"
      },
      "source": [
        "# import fiftyone.zoo as foz\n",
        "\n",
        "# my_dataset = foz.load_zoo_dataset(\n",
        "#     \"coco-2017\",\n",
        "#     split=\"validation\",\n",
        "#     classes=[\"cat\", \"dog\"],\n",
        "#     max_samples=50,\n",
        "#     shuffle=True,\n",
        "# )\n",
        "\n",
        "# # Print some information about the dataset\n",
        "# print(my_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krdoeaX-zRDi"
      },
      "source": [
        "> If your are using this `my_dataset`: please replace `coco_dataset` with `my_dataset`, and replace `ground_truth_detections` with `ground_truth` in all the following code.\n",
        "\n",
        "In addition to loading the COCO datasets themselves,  you could load your own datasets stored in [COCO format](https://cocodataset.org/#format-data) using [this guide](https://voxel51.com/docs/fiftyone/integrations/coco.html#loading-coco-formatted-data).\n",
        "\n",
        "### (4) Make predictions\n",
        "\n",
        "Install Tensorflow 2 Object Detection API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHIpgqor4JwW"
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver .\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtIE2cfz4PX5"
      },
      "source": [
        "Now, we will use the chosen pre-trained model to make predictions on our dataset.\n",
        "\n",
        "You could set `min_score_thresh` to other values (between 0 and 1) to allow more detections in or to filter out more detections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WkjSEKZyKqv"
      },
      "source": [
        "min_score_thresh=.30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROjw678WyKsu",
        "outputId": "c753a6a8-bcf3-49b5-b264-12dbbf04825c"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "# Choose a random subset of 25 samples to add predictions.\n",
        "# The larger the sample you choose, the longer it will take.\n",
        "predictions_view = coco_dataset.take(25, seed=51)\n",
        "\n",
        "# Get class list\n",
        "classes = coco_dataset.default_classes\n",
        "\n",
        "# Add predictions to samples\n",
        "with fo.ProgressBar() as pb:\n",
        "    for sample in pb(predictions_view):\n",
        "        # Load image\n",
        "        image_np = load_image_into_numpy_array(sample.filepath)\n",
        "\n",
        "        # Things to try:\n",
        "        # Convert image to grayscale\n",
        "        # Here we still expect the input image to have 3 channels\n",
        "        \n",
        "        # image_np = np.tile(\n",
        "        #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "        input_tensor = tf.convert_to_tensor(image_np)\n",
        "        \n",
        "        # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "        input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "        # input_tensor = np.expand_dims(image_np, 0)\n",
        "        detections = detect_fn(input_tensor)\n",
        "\n",
        "        # All outputs are batches tensors.\n",
        "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "        # We're only interested in the first num_detections.\n",
        "        num_detections = int(detections.pop('num_detections'))\n",
        "        detections = {key: value[0, :num_detections].numpy()\n",
        "                       for key, value in detections.items()}\n",
        "        detections['num_detections'] = num_detections\n",
        "\n",
        "        # detection_classes should be ints.\n",
        "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "        image_np_with_detections = image_np.copy()\n",
        "\n",
        "        result_detections=[]\n",
        "        for label, score, box in zip(detections['detection_classes'], detections['detection_scores'], detections['detection_boxes']):\n",
        "            # Convert to [top-left-x, top-left-y, width, height]            \n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            rel_box = [xmin, ymin, (xmax - xmin), (ymax - ymin)]\n",
        "\n",
        "            # result_detections.append({\"image_id\": 1, \"category_id\": label, \"bbox\": rel_box, \"score\": score})\n",
        "            if score>min_score_thresh:\n",
        "                result_detections.append(\n",
        "                    fo.Detection(\n",
        "                        label=classes[label],\n",
        "                        bounding_box=rel_box,\n",
        "                        confidence=score\n",
        "                    )\n",
        "                )\n",
        "                \n",
        "        # Save predictions to dataset\n",
        "        sample[\"mytest\"] = fo.Detections(detections=result_detections)\n",
        "        sample.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████████| 25/25 [25.9s elapsed, 0s remaining, 3.4 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbyhRLJzaLo"
      },
      "source": [
        "As our data is stored in COCO format, it can be loaded into FiftyOne and then visualised in this application. Let’s load `predictions_view` in the FiftyOne to visualize the predictions that we added:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "https://localhost:5151/polling?sessionId=7fad3853-80d0-4f15-b6b5-30eb9b6b48ad": {
              "data": "eyJtZXNzYWdlcyI6IFtdfQ==",
              "ok": true,
              "headers": [
                [
                  "access-control-allow-headers",
                  "x-requested-with"
                ],
                [
                  "content-type",
                  "text/html; charset=UTF-8"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "SmRG13jzrgwS",
        "outputId": "ab921f5f-ae83-4e8f-8cc2-13bf296f5483"
      },
      "source": [
        "session.view = predictions_view"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "#focontainer-1e660d4d-5f98-4151-bf4a-005b2113f1e2 {\n",
              "  position: relative;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-1e660d4d-5f98-4151-bf4a-005b2113f1e2 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-1e660d4d-5f98-4151-bf4a-005b2113f1e2:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-1e660d4d-5f98-4151-bf4a-005b2113f1e2 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-1e660d4d-5f98-4151-bf4a-005b2113f1e2\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-1e660d4d-5f98-4151-bf4a-005b2113f1e2\">\n",
              "      <button id=\"foactivate-1e660d4d-5f98-4151-bf4a-005b2113f1e2\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zHu9cljzkFT"
      },
      "source": [
        "## 3 Evaluation\n",
        "\n",
        "Now that we have samples with ground truth and predicted objects, let’s use FiftyOne to evaluate the quality of the detections. FiftyOne provides a powerful [evaluation API](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html) that contains a collection of methods for performing evaluation of model predictions. Since we’re working with object detections here, we’ll use [detection evaluation](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html#detections).\n",
        "\n",
        "### (1) Definitions of different metrics\n",
        "\n",
        "Adapted from [here](https://github.com/rafaelpadilla/Object-Detection-Metrics#important-definitions) and [here](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173).\n",
        "\n",
        "#### True Positive, False Positive, False Negative and True Negative\n",
        "Some basic concepts used by the metrics:\n",
        "    \n",
        "- True Positive (TP): A correct detection. Detection with IOU ≥ threshold\n",
        "- False Positive (FP): A wrong detection. Detection with IOU < threshold\n",
        "- False Negative (FN): A ground truth not detected\n",
        "- True Negative (TN): Does not apply. It would represent a corrected misdetection. In the object detection task there are many possible bounding boxes that should not be detected within an image. Thus, TN would be all possible bounding boxes that were corrrectly not detected (so many possible boxes within an image). That's why it is not used by the metrics.\n",
        "\n",
        "*threshold*: depending on the metric, it is usually set to 50%, 75% or 95%.\n",
        "\n",
        "#### Precision, Recall and F1\n",
        "\n",
        "- Precision is the ability of a model to identify only the relevant objects. It is the percentage of correct positive predictions.\n",
        "- Recall is the ability of a model to find all the relevant cases (all ground truth bounding boxes). It is the percentage of true positive detected among all relevant ground truths.\n",
        "\n",
        "Overall: `Percision = Cut the bullshit`, `Recall = FOMO`. Here are their mathematical definitions:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1068/1*EXa-_699fntpUoRjZeqAFQ.jpeg\"/>\n",
        "\n",
        "\n",
        "#### Micro/Macro/Weighted Average\n",
        "- Micro-average: aggregates the contributions of all classes to compute the average metric.  Basically, every sample-class pair contributes equally to the overall metric.\n",
        "- Macro-averaged: compute the metric independently for each class and then take the average, hence all classes equally contribute to the final averaged metric.\n",
        "- Weighted-averaged: each classes’s contribution to the average is weighted by its sample size.\n",
        "\n",
        "See [here](https://tomaxent.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/) for an example of their calculation\n",
        "\n",
        "#### Intersection Over Union (IOU)\n",
        "\n",
        "Intersection Over Union (IOU) is a measure based on Jaccard Index that evaluates the overlap between two bounding boxes. It requires a ground truth bounding box  and a predicted bounding box . By applying the IOU we can tell if a detection is valid (True Positive) or not (False Positive).\n",
        "\n",
        "IOU is given by the overlapping area between the predicted bounding box and the ground truth bounding box divided by the area of union between them:  \n",
        "\n",
        "$$IOU=\\frac{area(B_p \\cap B_{gt})}{area(B_p \\cup B_{gt})}$$\n",
        "\n",
        "The image below illustrates the IOU between a ground truth bounding box (in blue) and a detected bounding box (in red).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*FrmKLxCtkokDC3Yr1wc70w.png\" width=600/>\n",
        "<center>(image source: Jonathan Hui)</center>\n",
        "\n",
        "####  Mean Average Precision (mAP)\n",
        "\n",
        "mAP is a fairly complex metric (see [this blog](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) for a more detailed explanation). \n",
        "\n",
        "In short, mAP is calculated by:\n",
        "    \n",
        "1. Matching prediction with ground truth object if they overlap above some IoU value. The COCO evaluation protocol introduces one additional step: mAP are averaged across a range of 10 IoU thresholds from 0.5 to 0.95 in increments of 0.05. \n",
        "2. Computing the number of TP, FP, and FN separately for each class and IoU threshold. Using these TP, FP, FN to generate a precision-recall curve.\n",
        "3. Computing the average precision (AP) for each class and IoU threshold. Average Precision (AP) is the Area under the Precision-Recall Curve (AUC of PR-curve).\n",
        "4. For every class that contains at least one ground truth object, compute the AP by averaging the precision values over all 10 IoU thresholds. Then compute mAP by averaging the per-class AP values over all classes\n",
        "\n",
        "Refer to [mAP protocol](https://voxel51.com/docs/fiftyone/integrations/coco.html#map-protocol) for the specific mAP calculation steps.\n",
        "\n",
        "\n",
        "### (2) Running evaluation\n",
        "We can run evaluation on our samples via [`evaluate_detections( )`](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html?highlight=evaluate_detections#fiftyone.core.collections.SampleCollection.evaluate_detections). Note that this method is available on both the Dataset and DatasetView classes, which means that we can run evaluation on our `predictions_view` to assess the quality of predictions in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEJHjacEzb4l",
        "outputId": "73a0adf0-8fc9-4833-ebf0-d83e8a780026"
      },
      "source": [
        "# Evaluate the predictions in the model field of our `predictions_view`\n",
        "# with respect to the objects in the `ground_truth_detections` field\n",
        "\n",
        "results = predictions_view.evaluate_detections(\n",
        "    pred_field = \"mytest\", #the name of the field containing the predicted\n",
        "    gt_field=\"ground_truth_detections\", #the name of the field containing the ground truth\n",
        "    eval_key=\"eval\", \n",
        "    compute_mAP=True,\n",
        "    iou=0.5 #the IoU threshold to use to determine matches\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating detections...\n",
            " 100% |███████████████████| 25/25 [459.8ms elapsed, 0s remaining, 54.4 samples/s]      \n",
            "Performing IoU sweep...\n",
            " 100% |███████████████████| 25/25 [742.9ms elapsed, 0s remaining, 33.7 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9TZdQRznzy"
      },
      "source": [
        "### (3) Aggregate results\n",
        "\n",
        "The `results` object returned by the evaluation routine provides a number of convenient methods for analyzing our predictions. For example, let’s print a classification report for the top-10 most common classes in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V44Cvz3Dzb6r",
        "outputId": "d053a847-2c23-4914-b42a-e4146a82595c"
      },
      "source": [
        "# Get the 10 most common classes in the dataset by setting [:10]\n",
        "# You could remove [:10] to display all classes\n",
        "\n",
        "counts = coco_dataset.count_values(\"ground_truth_detections.detections.label\")\n",
        "classes_top10 = sorted(counts, key=counts.get, reverse=True)[:10]\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report(classes=classes_top10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      person       0.48      0.94      0.64        17\n",
            "       bench       0.38      1.00      0.56         5\n",
            "     handbag       0.33      0.25      0.29         4\n",
            "         car       0.00      0.00      0.00         1\n",
            "        boat       0.50      0.67      0.57         9\n",
            "      carrot       0.62      0.56      0.59         9\n",
            "dining table       0.33      0.50      0.40         4\n",
            "    suitcase       0.40      0.50      0.44         4\n",
            "        book       0.12      0.33      0.18         9\n",
            "     bicycle       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.38      0.65      0.48        62\n",
            "   macro avg       0.32      0.47      0.37        62\n",
            "weighted avg       0.41      0.65      0.49        62\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOpBiJZMnPdu"
      },
      "source": [
        "To visualise the detection performance for specific objects, we can plot their precision-recall (P-R) curves. The P-R curve is generated by sweeping through a range of confidence thresholds on the detections. The area under the P-R curve is averaged over all classes to compute mAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "tsBjgCA6nEDS",
        "outputId": "7108ec70-6f85-49a1-e805-9132bb2619e9"
      },
      "source": [
        "plot = results.plot_pr_curves(classes=[\"person\", \"bench\", \"car\"])\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"77def6ad-a8c5-4e0d-b9cd-36430490171b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"77def6ad-a8c5-4e0d-b9cd-36430490171b\")) {                    Plotly.newPlot(                        \"77def6ad-a8c5-4e0d-b9cd-36430490171b\",                        [{\"hovertemplate\": \"<b>class: %{text}</b><br>recall: %{x}<br>precision: %{y}<extra></extra>\", \"line\": {\"color\": \"#3366CC\"}, \"mode\": \"lines\", \"name\": \"person (AP = 0.548)\", \"text\": [\"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\"], \"type\": \"scatter\", \"x\": [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.96, 0.97, 0.98, 0.99, 1.0], \"y\": [0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.8400000000000001, 0.7625, 0.7625, 0.7625, 0.7625, 0.7625, 0.7625, 0.7275, 0.7275, 0.7275, 0.7275, 0.7275, 0.7275, 0.6675000000000001, 0.6675000000000001, 0.6675000000000001, 0.6675000000000001, 0.6675000000000001, 0.6675000000000001, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.6442857142857144, 0.6442857142857144, 0.6442857142857144, 0.6442857142857144, 0.6442857142857144, 0.5741071428571429, 0.5741071428571429, 0.5741071428571429, 0.5741071428571429, 0.5741071428571429, 0.5741071428571429, 0.4169642857142857, 0.4169642857142857, 0.4169642857142857, 0.4169642857142857, 0.4169642857142857, 0.4169642857142857, 0.4050595238095237, 0.4050595238095237, 0.4050595238095237, 0.4050595238095237, 0.4050595238095237, 0.4050595238095237, 0.34479166666666666, 0.34479166666666666, 0.34479166666666666, 0.34479166666666666, 0.34479166666666666, 0.34479166666666666, 0.20520833333333335, 0.20520833333333335, 0.20520833333333335, 0.20520833333333335, 0.20520833333333335, 0.20520833333333335, 0.20020833333333332, 0.20020833333333332, 0.20020833333333332, 0.20020833333333332, 0.20020833333333332, 0.20020833333333332, 0.15333333333333332, 0.15333333333333332, 0.15333333333333332, 0.15333333333333332, 0.15333333333333332, 0.15333333333333332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, {\"hovertemplate\": \"<b>class: %{text}</b><br>recall: %{x}<br>precision: %{y}<extra></extra>\", \"line\": {\"color\": \"#DC3912\"}, \"mode\": \"lines\", \"name\": \"bench (AP = 0.209)\", \"text\": [\"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\", \"bench\"], \"type\": \"scatter\", \"x\": [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.96, 0.97, 0.98, 0.99, 1.0], \"y\": [0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.27936507936507937, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.2571428571428572, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.19047619047619047, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556, 0.05555555555555556]}, {\"hovertemplate\": \"<b>class: %{text}</b><br>recall: %{x}<br>precision: %{y}<extra></extra>\", \"line\": {\"color\": \"#FF9900\"}, \"mode\": \"lines\", \"name\": \"car (AP = 0.000)\", \"text\": [\"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\"], \"type\": \"scatter\", \"x\": [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.96, 0.97, 0.98, 0.99, 1.0], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}],                        {\"margin\": {\"b\": 0, \"l\": 0, \"r\": 0, \"t\": 30}, \"shapes\": [{\"line\": {\"dash\": \"dash\"}, \"type\": \"line\", \"x0\": 0, \"x1\": 1, \"y0\": 1, \"y1\": 0}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"rgb(51,51,51)\"}, \"error_y\": {\"color\": \"rgb(51,51,51)\"}, \"marker\": {\"line\": {\"color\": \"rgb(237,237,237)\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"rgb(237,237,237)\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"rgb(51,51,51)\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"rgb(51,51,51)\"}, \"baxis\": {\"endlinecolor\": \"rgb(51,51,51)\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"rgb(51,51,51)\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}, \"colorscale\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"rgb(237,237,237)\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"rgb(217,217,217)\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"tickcolor\": \"rgb(237,237,237)\", \"ticklen\": 6, \"ticks\": \"inside\"}}, \"colorscale\": {\"sequential\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]], \"sequentialminus\": [[0, \"rgb(20,44,66)\"], [1, \"rgb(90,179,244)\"]]}, \"colorway\": [\"#F8766D\", \"#A3A500\", \"#00BF7D\", \"#00B0F6\", \"#E76BF3\"], \"font\": {\"color\": \"rgb(51,51,51)\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"rgb(237,237,237)\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"rgb(237,237,237)\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\"}, \"bgcolor\": \"rgb(237,237,237)\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"rgb(237,237,237)\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"rgb(237,237,237)\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"rgb(237,237,237)\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"fillcolor\": \"black\", \"line\": {\"width\": 0}, \"opacity\": 0.3}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\"}, \"bgcolor\": \"rgb(237,237,237)\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\"}}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\"}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"showgrid\": true, \"tickcolor\": \"rgb(51,51,51)\", \"ticks\": \"outside\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\"}}}, \"xaxis\": {\"constrain\": \"domain\", \"range\": [0, 1], \"title\": {\"text\": \"Recall\"}}, \"yaxis\": {\"constrain\": \"domain\", \"range\": [0, 1], \"scaleanchor\": \"x\", \"scaleratio\": 1, \"title\": {\"text\": \"Precision\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('77def6ad-a8c5-4e0d-b9cd-36430490171b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBWTEmkRzr1S"
      },
      "source": [
        "### (4) Evaluating model mAP\n",
        "\n",
        "We can also compute the mean average-precision (mAP) of our detector (all mAP calculations are performed according to the [COCO evaluation protocol](https://cocodataset.org/#detection-eval)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxtp43srzb89",
        "outputId": "2105548a-d634-42ba-d0cf-69654ccb3085"
      },
      "source": [
        "print(results.mAP())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4497866871508579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhiOtYFqzv4H"
      },
      "source": [
        "### (5) Sample-level analysis\n",
        "The evaluation routine also populated some new fields on our dataset that contain helpful information that we can use to evaluate our predictions at the sample-level.\n",
        "\n",
        "In particular, each sample now contains new fields:\n",
        "- `eval_tp`: the number of true positive (TP) predictions in the sample\n",
        "- `eval_fp`: the number of false positive (FP) predictions in the sample\n",
        "- `eval_fn`: the number of false negative (FN) predictions in the sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxRuSSzyzb_S",
        "outputId": "7b86db57-2fe3-489b-b2f1-7667d20259e8"
      },
      "source": [
        "# Our dataset's schema now contains `eval_*` fields\n",
        "print(coco_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:        2021.10.16.06.56.52\n",
            "Media type:  image\n",
            "Num samples: 50\n",
            "Persistent:  False\n",
            "Tags:        []\n",
            "Sample fields:\n",
            "    id:                      fiftyone.core.fields.ObjectIdField\n",
            "    filepath:                fiftyone.core.fields.StringField\n",
            "    tags:                    fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
            "    metadata:                fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
            "    ground_truth_detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
            "    ground_truth_coco_id:    fiftyone.core.fields.IntField\n",
            "    mytest:                  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
            "    eval_tp:                 fiftyone.core.fields.IntField\n",
            "    eval_fp:                 fiftyone.core.fields.IntField\n",
            "    eval_fn:                 fiftyone.core.fields.IntField\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O0wQuAizzsV"
      },
      "source": [
        "The individual predicted and ground truth objects also have fields populated on them describing the results of the matching process:\n",
        "- `eval`: whether the object is a TP/FP/FN\n",
        "- `eval_id`: the ID of the matching ground truth/predicted object, if any\n",
        "- `eval_iou`: the IoU between the matching objects, if any\n",
        "\n",
        "These extra fields were added because we provided the `eval_key` parameter to `evaluate_detections()`. If we had omitted this parameter, then no information would have been recorded on our samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K6mLtA2zcBi",
        "outputId": "8abd5fed-2765-4498-967f-b14b7f85f551"
      },
      "source": [
        "# Our detections have helpful evaluation data on them\n",
        "sample = predictions_view.first()\n",
        "print(sample.mytest.detections[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Detection: {\n",
            "    'id': '616a77ee36fd785cd314eec7',\n",
            "    'attributes': BaseDict({}),\n",
            "    'tags': BaseList([]),\n",
            "    'label': 'elephant',\n",
            "    'bounding_box': BaseList([\n",
            "        0.11572663486003876,\n",
            "        0.34526488184928894,\n",
            "        0.2370869666337967,\n",
            "        0.14691317081451416,\n",
            "    ]),\n",
            "    'mask': None,\n",
            "    'confidence': 0.8228263854980469,\n",
            "    'index': None,\n",
            "    'eval': 'tp',\n",
            "    'eval_id': '616a77b536fd785cd314ece9',\n",
            "    'eval_iou': 0.8585059423309278,\n",
            "}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBJfcVp2z3at"
      },
      "source": [
        "## 4 Exercise\n",
        "### Exercise 1\n",
        "Compare and contrast the results of the Faster R-CNN, SSD, and CenterNet. Also try swapping the \"backbone\" of a network (e.g., ResNet vs. MobileNet) to see if it changes the results. (Hints: change the `MODEL_DATE` and ` MODEL_NAME` in **2(1)-Download the model**)\n",
        "\n",
        "### Exercise 2\n",
        "Try changing `min_score_thresh` in **2(4)-Make predictions** to other values (between 0 and 1). How does this affect the results of detections? Why?\n",
        "\n",
        "Hint: This is where you need to look at\n",
        "```python\n",
        "for label, score, box in zip(detections['detection_classes'], detections['detection_scores'], detections['detection_boxes']):\n",
        "    # Convert to [top-left-x, top-left-y, width, height]            \n",
        "    ymin, xmin, ymax, xmax = box\n",
        "    rel_box = [xmin, ymin, (xmax - xmin), (ymax - ymin)]\n",
        "\n",
        "    if score > min_score_thresh:\n",
        "        result_detections.append(\n",
        "            fo.Detection(\n",
        "                label=classes[label],\n",
        "                bounding_box=rel_box,\n",
        "                confidence=score\n",
        "            )\n",
        "        )\n",
        "```\n",
        "\n",
        "### Exercise 3\n",
        "Modify some of the input images and see if detection still works. For example, converting the images to grayscale (just uncomment the relevant portions of code in **2(4)-Make predictions**), or splitting your own sub-dataset from the COCO (see **2(3)-Optional**).\n",
        "\n",
        "*Answer: Open-ended question. This will help you find out where the model does not perform well, or what kind of details it usually captures in the dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<marquee style='width: 80%; color: white; font-size:30px;' scrollamount=5 direction=\"down\"\n",
        "  width=\"300\"\n",
        "  height=\"100\"\n",
        "  behavior=\"alternate\"\n",
        "  style=\"border:solid\"><marquee behavior=\"alternate\" scrollamount=5.5><b>Congratulations, you have reached the ending path of your course!</b></marquee></marquee>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "9TiNJTLR5B-i",
        "outputId": "4ba0497b-4b69-4551-ea5b-a333111f65eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<marquee style='width: 80%; color: white; font-size:30px;' scrollamount=5 direction=\"down\"\n",
              "  width=\"300\"\n",
              "  height=\"100\"\n",
              "  behavior=\"alternate\"\n",
              "  style=\"border:solid\"><marquee behavior=\"alternate\" scrollamount=5.5><b>Congratulations, you have reached the ending path of your course!</b></marquee></marquee>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After this, you will:\n",
        "- Know about image processing techniques, and image representations\n",
        "- Know about how deep learning is used in computer vision\n",
        "- Know that it's called CNN, not some shit like `iMagE Ai`\n",
        "- Know how Midjourney, DALL-E, etc. works (See week 10)\n",
        "- Be proud that you had me as your sick ass tutor (jk, would be great to have your feedback. It will be very meaningful to me)\n",
        "- Able to flex with your friends about all of the above\n",
        "\n",
        "## For the exam:\n",
        "- Don't try to aim for 100%, aim to do as best as you can (you can thank me later, this applies to almost every UniMelb exam). Be strategic!\n",
        "- No coding, theoretical questions will be asked. Some examples:\n",
        "  - What happens when you change parameter `x` when doing `xxx` method?\n",
        "  - Why do we need to do `y` when doing `yyy`?\n",
        "  - Does `aaa` variation of `bbb` method works on `ccc` type of images? Why?\n",
        "  - What is the correct `ggg` representation of this image?\n",
        "  - True/False stuffs\n",
        "- Some mathy questions (very basic maths):\n",
        "  - Camera geometry\n",
        "  - CNN parameters/multiplications\n",
        "  - Object detection (ROI, metrics like percision, recall, etc.)\n",
        "  - And probably more (but no stuffs like 'proof that derivative of `kkk` is equal to sum squared of `lll`' though)\n",
        "- Open-ended question (might be a long one - you can probably prepare some templates):\n",
        "  - What model would be suitable for this?\n",
        "  - How would you set up your datasets?\n",
        "  - A method to handle these types of problem?\n",
        "- In case you need revision, I stored the notebook solutions here: https://github.com/tuankhoin/COMP90086-Practical-Solutions . Give it a star ⭐ to make me look cool on GitHub 👌!\n",
        "\n",
        "For now, get on the beers 🍺. After the exams, get on another beer 🍺! Good luck everyone! May your next journeys have more joy and less (UniMelb) pain."
      ],
      "metadata": {
        "id": "GlcNt4uBwHlx"
      }
    }
  ]
}