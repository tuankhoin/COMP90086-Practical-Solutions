{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymKUN26eQPQn"
   },
   "source": [
    "# COMP90086 Workshop 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZe6RbMPQPQp"
   },
   "source": [
    "In this workshop, you will have some practice of Hough Transform and Feature matching.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Hough Transform\n",
    "    - How Hough Transform works\n",
    "    - Implement Hough transform to detect lines\n",
    "\n",
    "- Feature matching\n",
    "    - Harris corning detection\n",
    "    - SIFT\n",
    "    - FLANN based Matcher\n",
    "    - Feature matching + homography to find objects\n",
    "    \n",
    "- Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ak6ygAD_QRNi",
    "outputId": "46b08486-51b8-4f23-f769-52250bd9e349"
   },
   "outputs": [],
   "source": [
    "## For colab user, run\n",
    "\n",
    "#! pip install opencv-contrib-python==4.5.2.52\n",
    "\n",
    "## After the update, You must restart the runtime in order to use newly installed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxSRU-VSQPQq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2  \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkmjeX8GQPQq"
   },
   "source": [
    "# Hough Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WN1wPCvQPQq"
   },
   "source": [
    "##  (1) How Hough Transform works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "2NEqneMIQPQr",
    "outputId": "fe9d3481-58db-401a-ff82-084decd8aa26"
   },
   "source": [
    "<center>Implementing a mapping between a Cartesian coordinate system and its parameter space.</center>\n",
    "<center>y=kx+b --> b=-kx+y</center>\n",
    "<center>Lines with slope k that do not exist cannot be described under the parameter space (The corresponding lines of M and Z in parameter space are parallel)</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/Hough.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Cartesian coordinates mapped to a parameter space under a polar coordinate system</center>\n",
    "<center>for each point (x0,y0) --> rθ=x0⋅cosθ+y0⋅sinθ</center>\n",
    "<center>each pair (rθ,θ) represents each line that passes by (x0,y0).</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/hough_space.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>for lines parallel to the X or Y axis can also be well mapped to the parameter space to ensure that they have intersection points</center>\n",
    "<img style=\"float:  \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/hough_a.jpeg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvR-DaF-QPQr"
   },
   "source": [
    "## (2) Implement Hough transform to detect lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwnOLS5LQPQs"
   },
   "source": [
    "First, we detect the edges of the image by using the Canny edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "jgt9xgIHQPQs",
    "outputId": "3c6fd15b-b6da-4ce0-f0a2-bb3f319080f6"
   },
   "outputs": [],
   "source": [
    "#read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "gray = cv2.imread(os.path.join(rootpath, \"canny_im.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Canny edge detection with OpenCV\n",
    "edge_img = cv2.Canny(gray,100,150,apertureSize=3,L2gradient=True) #two thresholds in Hysteresis Thresholding and Aperture size of the Sobel\n",
    "\n",
    "# Set the Figure size of plotting\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(gray, cmap='gray')  \n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(edge_img, cmap='gray')  \n",
    "plt.title('Canny edge map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io7oItNjQPQt"
   },
   "source": [
    "Then, we apply Standard Hough Line Transform using the OpenCV functions, cv2.HoughLines(image, rho, theta, threshold) with the following arguments:\n",
    "\n",
    "- image: Output of the edge detector, a single-channel binary source image.\n",
    "\n",
    "- rho: The resolution of the parameter ρ in pixels. We use 1 pixel.\n",
    "\n",
    "- theta: The resolution of the parameter θ in radians. We use 1 degree.\n",
    "\n",
    "- threshold: The minimum number of intersections to \"*detect*\" a line. In other words, only those lines are returned that get enough votes (>threshold).\n",
    "\n",
    "\n",
    "For further understanding, see:\n",
    "- [Hough Transform](http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm).\n",
    "- [cv2.HoughLines( )](https://docs.opencv.org/4.5.2/dd/d1a/group__imgproc__feature.html#ga46b4e588934f6c8dfd509cc6e0e4545a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:  \" src=\"https://docs.opencv.org/4.5.2/Hough_Lines_Tutorial_Theory_0.jpg\" width=300>\n",
    "(Image source: OpenCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the threshold and see what happens!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgIQYZKKQPQt"
   },
   "outputs": [],
   "source": [
    "#Implement Hough transform to detect lines in a Canny edge map \n",
    "\n",
    "# Finds lines in a binary image using the standard Hough transform\n",
    "lines = cv2.HoughLines(edge_img, 1, np.pi/180, thres)  \n",
    "# This function outputs vector of detected lines. \n",
    "# Each line is represented by a vector (ρ,θ). \n",
    "# Whereas ρ/rho is the distance from the coordinate origin (0,0) \n",
    "# and θ/theta is the line rotation angle in radians ( 0∼vertical line,π/2∼horizontal line )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_nmcLsPQPQt"
   },
   "source": [
    "Now, we display the result by drawing detected lines on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "JS33RVWrQPQt",
    "outputId": "815f60fe-d11f-4e6c-c262-15ea96c599ee"
   },
   "outputs": [],
   "source": [
    "# (Optional) To better distinguish the detected lines from the background, \n",
    "# we will draw the lines in colour. \n",
    "# Therefore, we first convert the grayscale map to RGB format.\n",
    "color_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB) \n",
    "\n",
    "# Display by drawing the lines\n",
    "for line in lines:\n",
    "    rho,theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    pt1 = (int(x0 + 1000*(-b)),int(y0 + 1000*(a)))\n",
    "    pt2 = (int(x0 - 1000*(-b)),int(y0 - 1000*(a)))\n",
    "    # Draws a line segment connecting two points, colour=(255,0,0) and thickness=2.\n",
    "    cv2.line(color_img,pt1,pt2,(255,0,0),2)\n",
    "\n",
    "plt.imshow(color_img) \n",
    "plt.title(\"Detected Lines (in red)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMWTAkzNQPQu"
   },
   "source": [
    "# Feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners (E and F) are easier to isolate and locate in the image than other features, which makes them better features to detect. \n",
    "\n",
    "(Image: A. Mordvintsev and A. K. Revision). \n",
    "<img style=\"float: \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/building_corner.jpeg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: \" src=\"https://raw.githubusercontent.com/saraao/COMP90086_image/main/corner.jpeg\" width=500>\n",
    "(Image: Moravec 1980)\n",
    "\n",
    "Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation, and illumination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2_di2wSQPQu"
   },
   "source": [
    "## (1) Harris corner detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNGIbXFvQPQu"
   },
   "source": [
    "We apply Harris Corner Detection using the OpenCV functions, cv2.cornerHarris(image, blockSize, ksize, k) with the following arguments:\n",
    "\n",
    "- img: A single-channel float32 input image.\n",
    "- blockSize: Neighborhood size for corner detection.\n",
    "- ksize: Aperture parameter for the Sobel operator.\n",
    "- k: Harris detector free parameter in the equation. Often between 0.04 and 0.06.\n",
    "\n",
    "For further understanding, see:\n",
    "- [cv2.cornerHarris( )](https://docs.opencv.org/4.5.2/dc/d0d/tutorial_py_features_harris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "mtK8yUyuQPQu",
    "outputId": "3c053515-03a3-4346-de24-3fe989211f2d"
   },
   "outputs": [],
   "source": [
    "# Read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "img = cv2.imread(os.path.join(rootpath, \"central.png\"))\n",
    "\n",
    "# detector parameters\n",
    "block_size = 2\n",
    "sobel_size = 3\n",
    "k = 0.04\n",
    "\n",
    "# Convert the colour from BGR to RGB for display\n",
    "color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the colour to grayscale & 32-bit float\n",
    "gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray_img = np.float32(gray_img)\n",
    "\n",
    "# Detect corners with custom detector parameters\n",
    "dst = cv2.cornerHarris(gray_img, block_size, sobel_size, k)\n",
    "\n",
    "# Dilate corner image to enhance corner points, not necessary\n",
    "#kernel = np.ones((3, 3), np.uint8) # The larger the size of the kernel, the greater the dilation\n",
    "#dst = cv2.dilate(dst, kernel)\n",
    "# Try to run the above codes to see what happens on the “Detected corners” map.\n",
    "\n",
    "# Create a copy of the image and draw corners on it\n",
    "corner_image = np.copy(color_img)\n",
    "\n",
    "# Vary the threshold according to the image and the number of corners you want to detect\n",
    "# The corners are drawn on the image if they pass the threshold\n",
    "thresh = 0.1\n",
    "corner_image[dst>thresh*dst.max()]=[0,255,0] # marking the corners in Green\n",
    "# Try to change the threshold to see what happens.\n",
    "\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(color_img)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(dst, cmap='gray')  \n",
    "plt.title('Detected corners')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(corner_image)\n",
    "plt.title('Display strong corners on the original image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr-Q4dpvQPQv"
   },
   "source": [
    "## (2) SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_Vkl36QQPQv"
   },
   "source": [
    "#### Harris corner detection is not scale invariant, so it is not applicable when the scale of the image changes. To find scale-invariant features, an algorithm called Scale Invariant Feature Transform (SIFT) was proposed by D.Lowe.\n",
    "\n",
    "<img style=\"float: \" src=\"https://miro.medium.com/max/1400/1*nkJt5BX6WJEDbVAl-fOE0A.png\" width=500>\n",
    "\n",
    "(Image Source: These images appears in many places, including [here](https://medium.com/jun94-devpblog/cv-12-scale-invariant-local-feature-extraction-2-harris-laplace-170d48ee1bf1))\n",
    "\n",
    "Harris corner point detection is rotationally invariant, but not scale invariant. As shown in the figure below, corner points at small scales may be assumed to be edges when they are zoomed in.\n",
    "\n",
    "\n",
    "For further understanding, see:\n",
    "- [SIFT](https://docs.opencv.org/4.5.2/da/df5/tutorial_py_sift_intro.html)\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [cv2.pyrDown( )](https://docs.opencv.org/4.5.2/d4/d86/group__imgproc__filter.html#gaf9bba239dfca11654cb7f50f889fc2ff)\n",
    "- [cv2.getRotationMatrix2D( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326)\n",
    "- [cv2.warpAffine( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983)\n",
    "- [cv2.drawKeypoints( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "teyo-JgWQPQv",
    "outputId": "336f3eb1-ee60-40a4-c0b3-c12d8315237f"
   },
   "outputs": [],
   "source": [
    "# Check your cv2 version\n",
    "cv2.__version__  \n",
    "# Note that these were previously only available in the opencv contrib repo, \n",
    "# but the patent expired in the year 2020. So they are now included in the main repo. \n",
    "\n",
    "# For those who are using OpenCV versions released before 2020 (e.g. cv2.__version__ == 4.2.x ), \n",
    "# they might got \"AttributeError: module 'cv2' has no attribute 'SIFT_create' \".  \n",
    "\n",
    "# To solve this problem, one possible solution is to uninstall Anaconda and reinstall to the latest version.\n",
    "# Then follow the Week 1 guide “Getting set up for workshops”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apX5K9GmQPQv"
   },
   "source": [
    "Display original image and scene image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "SmxfDHQrQPQv",
    "outputId": "af6d5811-90d7-428b-d924-eb7241b612cf"
   },
   "outputs": [],
   "source": [
    "# Read in images from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "gray = cv2.imread(os.path.join(rootpath, 'box.png'),cv2.IMREAD_GRAYSCALE)\n",
    "scene_gray = cv2.imread(os.path.join(rootpath, 'box_in_scene.png'),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Optional: Create a modified image by adding scale invariance and rotation invariance\n",
    "\n",
    "#scene_gray = cv2.pyrDown(gray) #blurs an image and downsamples it\n",
    "#rows, cols = scene_gray.shape[:2] #in case this is not a greyscale image\n",
    "#rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1) #calculate an affine matrix of 2D rotation\n",
    "#scene_gray = cv2.warpAffine(scene_gray, rotation_matrix, (cols, rows)) #apply an affine transformation to image\n",
    "\n",
    "\n",
    "# Display original image and scene image\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(gray, cmap='gray')  \n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(scene_gray, cmap='gray')  \n",
    "plt.title('Scene Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "Si7bjF2GQPQv",
    "outputId": "c23ff842-65b7-48ce-e65c-0c491094dcec"
   },
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create() # if cv2 version >= 4.4.0 \n",
    "# sift = cv2.xfeatures2d.SIFT_create() # if cv2 version = 4.3.x \n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(gray,None)\n",
    "kp2, des2 = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "# Draws the small circles on the locations of keypoints without size\n",
    "kp1_without_size = cv2.drawKeypoints(gray,kp1,None\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "kp2_without_size = cv2.drawKeypoints(scene_gray,kp2,None\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "\n",
    "# Draws a circle with the size of each keypoint and show its orientation\n",
    "kp1_with_size = cv2.drawKeypoints(gray,kp1,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp2_with_size = cv2.drawKeypoints(scene_gray,kp2,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "\n",
    "# Display images with&without the size of keypoints \n",
    "plt.subplots(figsize=(15, 10)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(kp1_without_size, cmap='gray')  \n",
    "plt.title('Original Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(kp2_without_size, cmap='gray')  \n",
    "plt.title('Scene Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(kp1_with_size, cmap='gray')  \n",
    "plt.title('Original Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(kp2_with_size, cmap='gray')  \n",
    "plt.title('Scene Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"Number of keypoints detected in the original image: \", len(kp1))\n",
    "print(\"Number of keypoints detected in the Scene image: \", len(kp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oEtxxhlQPQw"
   },
   "source": [
    "#### Now we've got the keypoints, descriptors and so on. Now we are going to see how to match keypoints in different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvlAq3spQPQw"
   },
   "source": [
    "## (3) FLANN based Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkBKqRqdQPQw"
   },
   "source": [
    "#### FLANN (Fast Library for Approximate Nearest Neighbors) contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. \n",
    "\n",
    "When performing matching, the KNN algorithm is generally used to find the two nearest neighbours. If the ratio between the closest and second closest is greater than a given value, then we keep this closest value and consider it and its matching point as a good match.\n",
    "\n",
    "For further understanding, see:\n",
    "\n",
    "- [FLANN based Matcher](https://docs.opencv.org/4.5.2/dc/dc3/tutorial_py_matcher.html#flann-based-matcher)\n",
    "\n",
    "More information on the functions used below, see:\n",
    "\n",
    "- [FlannBasedMatcher Class Reference](https://docs.opencv.org/4.5.2/dc/de2/classcv_1_1FlannBasedMatcher.html)\n",
    "- [cv2.drawMatchesKnn( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "2Dj51hrjQPQw",
    "outputId": "3d660e11-2af1-4ca3-f7ed-75721f67c0c9"
   },
   "outputs": [],
   "source": [
    "# FLANN parameters and initialize\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Create a mask to draw all good matches\n",
    "matchesMask = []\n",
    "\n",
    "# Store all good matches as per Lowe's Ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "        matchesMask.append([1,0]) # Match\n",
    "    else:\n",
    "        matchesMask.append([0,0]) # Mismatch\n",
    "       \n",
    "        \n",
    "# Draw all good matches\n",
    "draw_params = dict(#matchColor = (0,255,0),  #If you want a specific colour\n",
    "                   #singlePointColor = (255,0,0), #If you want a specific colour\n",
    "                    matchesMask = matchesMask,\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "good_matches = cv2.drawMatchesKnn(gray,kp1,scene_gray,kp2,matches,None,**draw_params)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.imshow(good_matches)\n",
    "plt.title('All good matches')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print total number of good matches between two images\n",
    "print(\"\\nNumber of good matches between two images: \", len(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ni5zuKQPQw"
   },
   "source": [
    "## (4) Feature matching + homography to find objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY438LrzQPQw"
   },
   "source": [
    "We have found a number of matching points in two images by following the steps above. How do we align one of the images with the other by rotating, transforming, etc.? This is where the homography matrix comes into play.\n",
    "\n",
    "The homography matrix has eight parameters. Since each corresponding pixel point can generate 2 equations (one x, one y), then only a minimum of four pixel points are enough to solve the Homography matrix.\n",
    "\n",
    "\n",
    "More information on the functions used below, see:\n",
    "\n",
    "- [cv2.findHomography( )](https://docs.opencv.org/4.5.2/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780)\n",
    "\n",
    "- [cv2.warpPerspective( )](https://docs.opencv.org/4.5.2/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87)\n",
    "\n",
    "- [cv2.drawMatches( )](https://docs.opencv.org/4.5.2/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "id": "S-3RO9m5QPQw",
    "outputId": "82854698-7731-44ef-a5a0-f2917531db2e"
   },
   "outputs": [],
   "source": [
    "# Now we set a condition that at least N matches (defined by MIN_MATCH_NUM) are required to find the object. \n",
    "MIN_MATCH_NUM = 4\n",
    "\n",
    "if len(good)>= MIN_MATCH_NUM:\n",
    "    # If enough matches are found, we extract the positions of the matched keypoints in both images. \n",
    "    # They are passed to find the perspective transformation. \n",
    "    \n",
    "    # Estimate homography between two images\n",
    "    ptsA = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    ptsB = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, status = cv2.findHomography(ptsA, \n",
    "                                   ptsB, \n",
    "                                   cv2.RANSAC, \n",
    "                                   ransacReprojThreshold = 5, \n",
    "                                   maxIters = 10) # try to change maxIters and see the effect\n",
    "    # Where H is the resulting single-strain matrix.\n",
    "    # status returns a list of feature points that represent successful matches.\n",
    "    # ptsA, ptsB are keypoints.\n",
    "    # The three parameters cv2.RANSAC, ransacReprojThreshold, maxIters are related to RANSAC.\n",
    "    # ransacReprojThreshold: Maximum reprojection error in the RANSAC algorithm to consider a point as an inlier. \n",
    "    # maxIters: The maximum number of RANSAC-based robust method iterations.\n",
    "    \n",
    "    success = status.ravel().tolist()\n",
    "    \n",
    "    # Draw detected template in scene image\n",
    "    imgOut = cv2.warpPerspective(scene_gray, H, (gray.shape[1],gray.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    \n",
    "    # Print total number of successful matches between two images\n",
    "    print(\"\\nNumber of successful matches between two images: \", success.count(1)) # Returns the number of 1 in the success list\n",
    "\n",
    "else:\n",
    "    # Otherwise, print that “Not enough matches are found”.\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_NUM) )\n",
    "    success = None\n",
    "\n",
    "\n",
    "# Draw our inliers (if successfully found the object) or all matching keypoints (if failed)\n",
    "draw_params = dict(#matchColor = (0,255,0), # draw in a specific colour\n",
    "                   #singlePointColor = (255,0,0), # draw in a specific colour\n",
    "                   matchesMask = success, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "success_matches = cv2.drawMatches(gray,kp1,scene_gray,kp2,good,None,**draw_params)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "plt.subplots(figsize=(15, 15)) \n",
    "\n",
    "if success == None:\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All matching keypoints')\n",
    "    plt.axis('off')\n",
    "    \n",
    "else:\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(success_matches)\n",
    "    plt.title('All successful matches')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(imgOut, 'gray')\n",
    "    plt.title('Display detected template in scene image')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8kFhztgQPQx"
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4tlgMT1QPQx"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "- Implement Canny edge detection and Hough transform to detect the grid in the checkerboard.\n",
    "- Change the threshold of the HoughLines function. What happens as you increase/decrease the threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH4JmnjLQPQx"
   },
   "outputs": [],
   "source": [
    "# Read in an image from a filepath as graycsale.\n",
    "rootpath='./'\n",
    "checkerboard = cv2.imread(os.path.join(rootpath, \"checkerboard.png\"),cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcfgFcRsQPQx"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt7TCwwMQPQx"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "- Use SIFT+RANSAC to match the close-up views of buildings to the larger scenes. Note that this may require changing the ratio used in the Lowe's ratio test step of FLANN matcher and/or the maxIters parameter of the RANSAC step. \n",
    "- What is the effect of changing these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "9bE1zGADQPQx",
    "outputId": "57ab6738-886c-4b19-80dc-8b34cd799f7d"
   },
   "outputs": [],
   "source": [
    "# Read in close-up views of buildings images and the larger scenes images as graycsale.\n",
    "rootpath='./'\n",
    "flinders1 = cv2.imread(os.path.join(rootpath, \"flinders1.png\"),cv2.IMREAD_GRAYSCALE) \n",
    "flinders2 = cv2.imread(os.path.join(rootpath, \"flinders2.png\"),cv2.IMREAD_GRAYSCALE) \n",
    "unimelb1 = cv2.imread(os.path.join(rootpath, \"unimelb1.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "unimelb2 = cv2.imread(os.path.join(rootpath, \"unimelb2.png\"),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Display original image and scene image\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(flinders1, cmap='gray')  \n",
    "plt.title('Close-up views of Flinders Station ')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(flinders2, cmap='gray')  \n",
    "plt.title('Larger scenes of Flinders Station')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(unimelb1, cmap='gray')  \n",
    "plt.title('Close-up views of Unimelb Old Arts Clock Tower')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(unimelb2, cmap='gray')  \n",
    "plt.title('Larger scenes of Unimelb Old Arts Clock Tower')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxeOVAZgQPQx"
   },
   "source": [
    "Unimelb Old Arts Clock Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LedcvX8GQPQx"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xdF0eFPQPQx"
   },
   "source": [
    "Flinders Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF7S9fXcQPQx"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNPyz3mRQPQx"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "\n",
    "- Implement the RANSAC algorithm step by step by filling in the three main steps of the RANSAC loop\n",
    "\n",
    "Adapted from: https://salzis.wordpress.com/2014/06/10/robust-linear-model-estimation-using-ransac-python-implementation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ck1cOAucQPQx"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    " \n",
    "# Ransac parameters\n",
    "ransac_iterations = 20  # number of iterations\n",
    "ransac_threshold = 3    # threshold\n",
    "ransac_ratio = 0.6      # ratio of inliers required to assert\n",
    "                        # that a model fits well to data\n",
    "\n",
    "# generate sparse input data\n",
    "n_samples = 500               # number of input points\n",
    "outliers_ratio = 0.4          # ratio of outliers\n",
    " \n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    " \n",
    "# generate samples\n",
    "x = 30*np.random.random((n_samples,n_inputs) )\n",
    " \n",
    "# generate line's slope (called here perfect fit)\n",
    "perfect_fit = 0.5*np.random.normal(size=(n_inputs,n_outputs) )\n",
    " \n",
    "# compute output\n",
    "y = np.dot(x,perfect_fit)\n",
    "\n",
    "# add a little gaussian noise\n",
    "x_noise = x + np.random.normal(size=x.shape)\n",
    "y_noise = y + np.random.normal(size=y.shape)\n",
    " \n",
    "# add some outliers to the point-set\n",
    "n_outliers = round(outliers_ratio*n_samples)\n",
    "indices = np.arange(x_noise.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "outlier_indices = indices[:n_outliers]\n",
    " \n",
    "x_noise[outlier_indices] = 30*np.random.random(size=(n_outliers,n_inputs))\n",
    " \n",
    "# gaussian outliers\n",
    "y_noise[outlier_indices] = 30*np.random.normal(size=(n_outliers,n_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDkvZv4aQPQx"
   },
   "source": [
    "These two functions will be called by the RANSAC loop: find_line_model() fits a line to sample points and dist_to_line() computes the distance from a sample point to the closest point on a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9tJY59PQPQx"
   },
   "outputs": [],
   "source": [
    "def find_line_model(points):\n",
    "    \"\"\" find a line model for the given points\n",
    "    :param points selected points for model fitting\n",
    "    :return line model\n",
    "    \"\"\"\n",
    " \n",
    "    # [WARNING] vertical and horizontal lines should be treated differently\n",
    "    #           here we just add some noise to avoid division by zero\n",
    " \n",
    "    # find a line model for these points\n",
    "    m = (points[1,1] - points[0,1]) / (points[1,0] - points[0,0] + sys.float_info.epsilon)  # slope (gradient) of the line\n",
    "    c = points[1,1] - m * points[1,0]                                     # y-intercept of the line\n",
    " \n",
    "    return m, c\n",
    "\n",
    "def find_dist_to_line(m, c, x0, y0):\n",
    "    \"\"\" find an intercept point of the line model with\n",
    "        a normal from point (x0,y0) to it, return\n",
    "        distance betwee point (x0, y0) and intercept\n",
    "    :param m slope of the line model\n",
    "    :param c y-intercept of the line model\n",
    "    :param x0 point's x coordinate\n",
    "    :param y0 point's y coordinate\n",
    "    :return intercept point\n",
    "    \"\"\"\n",
    " \n",
    "    # intersection point with the model\n",
    "    x = (x0 + m*y0 - m*c)/(1 + m**2)\n",
    "    y = (m*x0 + (m**2)*y0 - (m**2)*c)/(1 + m**2) + c\n",
    "    dist = math.sqrt((x - x0)**2 + (y - y0)**2)\n",
    " \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for plotting the RANSAC iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_plot(n, x, y, m, c, x_in=(), y_in=(), points=()):\n",
    "    \"\"\" plot the current RANSAC step\n",
    "    :param n      iteration\n",
    "    :param x      samples x\n",
    "    :param y      samples y\n",
    "    :param m      slope of the line model\n",
    "    :param c      shift of the line model\n",
    "    :param x_in   inliers x\n",
    "    :param y_in   inliers y\n",
    "    :param points picked up points for modeling\n",
    "    \"\"\"\n",
    "\n",
    "    line_width = 1.\n",
    "    line_color = '#0080ff'\n",
    "    title = 'iteration ' + str(n)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    # plot input points\n",
    "    plt.plot(x[:,0], y[:,0], marker='o', label='Input points', color='#00cc00', linestyle='None', alpha=0.4)\n",
    " \n",
    "    # draw the current model\n",
    "    plt.plot(x, m*x + c, 'r', label='Line model', color=line_color, linewidth=line_width)\n",
    " \n",
    "    # draw inliers, if provided\n",
    "    if len(x_in) > 0:\n",
    "        plt.plot(x_in, y_in, marker='o', label='Inliers', linestyle='None', color='#ff0000', alpha=0.6)\n",
    " \n",
    "    # draw points picked up for the modeling, if provided\n",
    "    if len(points) > 0:\n",
    "        plt.plot(points[:,0], points[:,1], marker='o', label='Picked points', color='#0000cc', linestyle='None', alpha=0.6)\n",
    " \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEJPfvfxQPQy"
   },
   "source": [
    "The main RANSAC loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack( (x_noise,y_noise) )\n",
    " \n",
    "ratio = 0.\n",
    "model_m = 0.\n",
    "model_c = 0.\n",
    " \n",
    "# perform RANSAC iterations\n",
    "for it in range(ransac_iterations):\n",
    " \n",
    "    # randomly sample N points\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # your code\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    # fit a line model to the sampled points\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # your code\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    # count the number of inliers num\n",
    "    # inliers are points whose distance to the line is less than ransac_threshold\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    # your code\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    # if this value of num is higher than previously saved value,\n",
    "    # save it, and save the current model parameters\n",
    "    if num/float(n_samples) > ratio:\n",
    "        ratio = num/float(n_samples)\n",
    "        model_m = m\n",
    "        model_c = c\n",
    " \n",
    "    print('   inlier ratio = ', num/float(n_samples))\n",
    "    print('  model_m = ', model_m)\n",
    "    print('  model_c = ', model_c)\n",
    " \n",
    "    # plot the current step with inliers and sample points\n",
    "    #ransac_plot(it, x_noise, y_noise, m, c, x_inliers, y_inliers, sample_points)\n",
    "    # plot the current step without showing inliers or sample points\n",
    "    ransac_plot(0, x_noise, y_noise, model_m, model_c)\n",
    " \n",
    "    # we are done in case we have enough inliers\n",
    "    if num > n_samples*ransac_ratio:\n",
    "        print('The model is found !')\n",
    "        break\n",
    "        \n",
    "# plot the final model\n",
    "ransac_plot(0, x_noise, y_noise, model_m, model_c)\n",
    " \n",
    "print('\\nFinal model:\\n')\n",
    "print('  ratio = ', ratio)\n",
    "print('  model_m = ', model_m)\n",
    "print('  model_c = ', model_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "worksheet07_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
