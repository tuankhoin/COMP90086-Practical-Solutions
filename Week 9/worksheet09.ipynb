{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d2db8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP90086 Workshop 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219953d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this workshop, you will see some demonstrations about texture and shape representation.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Texture and shape representation\n",
    "    - Image Quilting\n",
    "    - Parametric Texture Synthesis\n",
    "    - Neural Style Transfer\n",
    "    - Shape skeletons\n",
    "    - Active contours\n",
    "    \n",
    "- Bonus\n",
    "    - Texture Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35f047",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tqdm    \n",
    "## The tqdm library is used to display the progress bar. \n",
    "## Optional. If not to import tqdm, just modify the tqdm code in myutils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes importing some of custom packages\n",
    "# The functions in the custom package are called via myutils.xxx\n",
    "# e.g. the xx function needs to be called via myutils.xx \n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import myutils\n",
    "import skimage.io\n",
    "from skimage import data\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.morphology import medial_axis\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import gaussian\n",
    "from skimage.segmentation import active_contour\n",
    "from tqdm import tqdm \n",
    "\n",
    "rootpath='./Image/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539a6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first two algorithms (image quilting and texture synthesis) use different models for \"texture\". You do not need to know exactly how these two algorithms work. Instead, you are expected to visually compare and contrast the output of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8c10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1 Image quilting \n",
    "\n",
    "Image quilting is a method of synthesising a new image by stitching together small patches of existing images. It can be utilized for the following applications:\n",
    "\n",
    "    (1) Texture synthesis: synthesize a new texture sample by taking patches of existing texture and stitching them together\n",
    "    (2) Texture transfer: transfer the texture of one image onto another image\n",
    "\n",
    "For more information, see the original publication [Image Quilting for Texture Synthesis and Transfer(Efros & Freeman, 2001)](https://people.eecs.berkeley.edu/~efros/research/quilting.html)\n",
    "\n",
    "The following codes are adapted from https://github.com/axu2/image-quilting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5d8be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1 Texture synthesis\n",
    "Square blocks from the input texture are patched together to synthesize a new texture sample. We begin with randomly choose blocks.\n",
    "\n",
    "![Imagequilting](https://upload.wikimedia.org/wikipedia/commons/b/bc/Imagequilting.gif)\n",
    "<center>source:Drlanman</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cb121",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Display the process of texture synthesis\n",
    "texture = cv2.imread(os.path.join(rootpath, \"test.png\"))\n",
    "texture_rgb = cv2.cvtColor(texture, cv2.COLOR_BGR2RGB)\n",
    "patchsize = 20 # size of square patch for synthesis (overlap is 1/6 patch size)\n",
    "npatch = 3# size of output (npatches x npatches)\n",
    "showsteps = True # toggle image output for each step of synthesis\n",
    "texture_synth = myutils.quilt(texture_rgb, patchsize, (npatch, npatch), \"cut\", showsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da8fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Display the synthesized and original image\n",
    "plt.subplots(figsize=(10, 10))\n",
    "#Set a grid layout to place subplots within a figure.\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[texture_rgb.shape[1], texture_synth.shape[1]])\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(texture_rgb)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(texture_synth)\n",
    "plt.title('Synthesised')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7524129",
   "metadata": {},
   "source": [
    "### 1.2 Exercise\n",
    "\n",
    "What is the effect of changing patch size in the texture synthesis?\n",
    "\n",
    "Try a range of patch sizes for the \"brick.jpg\" and \"marble.jpg\" images. Note that if you use a very small patch size you may need to increase the number of patches (npatch) to get a large enough synthesis image to interpret the results. What range of patch sizes gives the best results for each image? What problems occur in the resulting synetheses if the patch size is too large or small in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589de2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read images\n",
    "brick = cv2.imread(os.path.join(rootpath, \"brick.jpg\"))\n",
    "brick_rgb = cv2.cvtColor(brick, cv2.COLOR_BGR2RGB)\n",
    "marble = cv2.imread(os.path.join(rootpath, \"marble.jpg\"))\n",
    "marble_rgb = cv2.cvtColor(marble, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Optional: Resize images for faster synthesis (original size = 256 x 256)\n",
    "brick_rgb = cv2.resize(brick_rgb, (128, 128))\n",
    "marble_rgb = cv2.resize(marble_rgb, (128, 128))\n",
    "\n",
    "# Synthesize texture\n",
    "# your code\n",
    "\n",
    "\n",
    "# Display the synthesized and original image\n",
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88187c4",
   "metadata": {},
   "source": [
    "## 2 Parametric texture synthesis\n",
    "\n",
    "Different from the previous algorithm, which considers texture as repeated image patch, this algorithm models texture as a sample from a statistical distribution over image features. For more information, see the original publication [A Parametric Texture Model based on Joint Statistics of Complex Wavelet Coefficients(Portilla & Simoncelli, 2000)](http://www.cns.nyu.edu/~eero/ABSTRACTS/portilla99-abstract.html).\n",
    "\n",
    "The following codes are adapted from https://github.com/TetsuyaOdaka/texture-synthesis-portilla-simoncelli\n",
    "\n",
    "You can run this algorithm on colour or grayscale images. The colour version takes about 3x longer to run than the grayscale version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67553255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in images\n",
    "pebbles =  cv2.imread(os.path.join(rootpath, \"pebbles.jpg\"))\n",
    "pebbles2 = cv2.imread(os.path.join(rootpath, \"pebbles_gray.jpg\"))\n",
    "\n",
    "pebbles_rgb = cv2.cvtColor(pebbles, cv2.COLOR_BGR2RGB)\n",
    "pebbles_gray = cv2.cvtColor(pebbles2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display the loaded images\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pebbles_rgb)\n",
    "plt.title('Pebbles')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pebbles_gray, cmap='gray')\n",
    "plt.title('Pebbles(gray)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd353f13",
   "metadata": {},
   "source": [
    "### 2.1 Colour texture synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute RGB colour version\n",
    "myutils.run_texture_synthesis(num_neighbor= 7, #local neighborhood\n",
    "                              orig_img='./Image/pebbles.jpg', #Original RGB image\n",
    "                              num_depth= 5, #depth of steerable pyramid\n",
    "                              num_ori= 4, #orientation of steerable pyramid\n",
    "                              num_iter= 25, #number of iterations\n",
    "                              out_dir='output_rgb') #Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the step-by-step results\n",
    "output_rgb_lst = os.listdir('output_rgb')\n",
    "output_rgb_lst.sort()\n",
    "for i in output_rgb_lst:\n",
    "    output_rgb_img = cv2.imread(\"output_rgb/\"+i)\n",
    "    output_rgb = cv2.cvtColor(output_rgb_img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(output_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa99149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original and final synthesis\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pebbles_rgb)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(output_rgb)\n",
    "plt.title('Synthesised')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9d66c",
   "metadata": {},
   "source": [
    "### 2.2 Grayscale texture synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Gray Scale Version\n",
    "myutils.run_texture_synthesis_g(num_neighbor= 7, #local neighborhood\n",
    "                              orig_img= './Image/pebbles_gray.jpg', #Original grayscale image\n",
    "                              num_depth= 5, #depth of steerable pyramid\n",
    "                              num_ori= 4, #orientation of steerable pyramid\n",
    "                              num_iter= 25, #number of iterations\n",
    "                              out_dir='output_gray') #Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b330d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the step-by-step results\n",
    "output_gray_lst = os.listdir('output_gray')\n",
    "output_gray_lst.sort()\n",
    "for i in output_gray_lst:\n",
    "    output_gray_img = cv2.imread(\"output_gray/\"+i, cv2.IMREAD_GRAYSCALE)\n",
    "    plt.imshow(output_gray_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original and final synthesis\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pebbles_gray, cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(output_gray_img, cmap='gray')\n",
    "plt.title('Synthesised')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302cf9e",
   "metadata": {},
   "source": [
    "### 2.3 Exercise\n",
    "\n",
    "Run the parametric synthesis on the \"brick.jpg\" and \"water.jpg\" textures. What problems (if any) do you observe in the resulting syntheses? Which image is more easily synthesized by the parametric approach and why?\n",
    "\n",
    "**Note:** for faster results, you may wish to use the grayscale version of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcaad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "use_color = 0\n",
    "if use_color:\n",
    "    brick = cv2.imread(os.path.join(rootpath, \"brick.jpg\"))\n",
    "    brick_orig = cv2.cvtColor(brick, cv2.COLOR_BGR2RGB)\n",
    "    water = cv2.imread(os.path.join(rootpath, \"water.jpg\"))\n",
    "    water_orig = cv2.cvtColor(water, cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    brick = cv2.imread(os.path.join(rootpath, \"brick_gray.jpg\"))\n",
    "    brick_orig = cv2.cvtColor(brick, cv2.COLOR_BGR2GRAY)\n",
    "    water = cv2.imread(os.path.join(rootpath, \"water_gray.jpg\"))\n",
    "    water_orig = cv2.cvtColor(water, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Execute synthesis\n",
    "# your code\n",
    "\n",
    "# Display the original and final synthesis\n",
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f7548",
   "metadata": {},
   "source": [
    "## 3 Neural style transfer\n",
    "\n",
    "Neural style transfer allows two images (one with content and one with style) to compose new artwork.\n",
    "\n",
    "- [Online demo](https://tenso.rs/demos/fast-neural-style/)\n",
    "\n",
    "    This online demo allows faster demonstration of style-transfer neural network.\n",
    "\n",
    "### 3.1 Exercise\n",
    "\n",
    "- [An implementation of neural style transfer (view in Google Colab)](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/neural_style_transfer.ipynb)\n",
    "\n",
    "    Please refer to the Neural style transfer tutorial from [Keras](https://keras.io/examples/generative/neural_style_transfer/) for a detailed implement. Note that it may take about 10 minutes to run all the code on the Google Colab with **free GPU**. To speed up, use a smaller `iterations` in `The training loop`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a9050",
   "metadata": {},
   "source": [
    "## 4 Shape skeletons\n",
    "\n",
    "The topological skeleton of a shape is the thinnest possible version of a shape, composed of points equidistant from the shape boundaries. There are many different ways to compute shape skeletons, but this method uses the medial axis method based on the [grassfire transform](https://en.wikipedia.org/wiki/Grassfire_transform) shown in lecture. The shape skeleton plus the distance from the skeleton pixel to the shape boundary is a complete representation of the shape.\n",
    "\n",
    "For more information on the functions used below, see:\n",
    "- [skimage.morphology.medial_axis( )](https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.medial_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3259173",
   "metadata": {},
   "source": [
    "`from skimage.morphology import skeletonize`\n",
    "\n",
    "[Skeletonization](https://scikit-image.org/docs/dev/auto_examples/edges/plot_skeleton.html) reduces binary objects to 1 pixel wide representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image\n",
    "image = skimage.io.imread(fname=os.path.join(rootpath, \"T.png\"))\n",
    "# Skeletonisation requires a binary image (1=shape pixel, 0=background)\n",
    "binary_image = (image > 0)\n",
    "skel = skeletonize(binary_image)\n",
    "\n",
    "# Compute the medial axis (skeleton) and the distance transform\n",
    "skel, distance = medial_axis(image, return_distance=True)\n",
    "\n",
    "# Display the image, skeleton, and distance transform\n",
    "plt.subplots(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(binary_image, cmap='gray')\n",
    "plt.title('Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(skel, cmap='gray')\n",
    "plt.title('Skeleton')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(distance)\n",
    "plt.title('Distance transform')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ccab3",
   "metadata": {},
   "source": [
    "### 4.1 Exercise\n",
    "\n",
    "Write a method to reconstruct the shape using the skeleton and distance transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271566d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367ebaa",
   "metadata": {},
   "source": [
    "## 5 Active contours\n",
    "The active contour model, also called snakes, is a method to fit open or closed splines to lines or edges in an image [1]. It works by minimising an energy that is in part defined by the image and part by the spline's shape: length and smoothness. The minimization is done implicitly in the shape energy and explicitly in the image energy.\n",
    "\n",
    "[1] *Snakes: Active contour models*. Kass, M.; Witkin, A.; Terzopoulos, D.\n",
    "       International Journal of Computer Vision 1 (4): 321 (1988).\n",
    "       :DOI:`10.1007/BF00133570`\n",
    "\n",
    "\n",
    "Adapted from [Active Contour Model â€” skimage v0.19.0.dev0 docs](https://scikit-image.org/docs/dev/auto_examples/edges/plot_active_contours.html)\n",
    "\n",
    "\n",
    "#### (1) Segment the face of a person from the rest of an image by fitting a closed curve to the edges of the face\n",
    "\n",
    "We initialize a circle around the astronaut's face and use the default boundary\n",
    "condition ``boundary_condition='periodic'`` to fit a closed curve. The default\n",
    "parameters ``w_line=0, w_edge=1`` will make the curve search towards edges,\n",
    "such as the boundaries of the face.\n",
    "\n",
    "For more information on the functions used below, see:\n",
    "- [skimage.segmentation.active_contour( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.active_contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get astronaut from skimage.data in grayscale\n",
    "img = rgb2gray(data.astronaut())\n",
    "\n",
    "# Check skimage version because the active contours method changed in v18\n",
    "print(skimage.__version__)\n",
    "version_text = (skimage.__version__).split('.')\n",
    "version = int(version_text[1])\n",
    "current = 18\n",
    "\n",
    "# Draw a circle around the astronaut's head to initialize the snake\n",
    "s = np.linspace(0, 2*np.pi, 400) # init will calculate 400 such points\n",
    "if version < current:\n",
    "    x = 220 + 100*np.cos(s)\n",
    "    y = 100 + 100*np.sin(s)\n",
    "    init = np.array([x, y]).T\n",
    "else:\n",
    "    r = 100 + 100*np.sin(s)\n",
    "    c = 220 + 100*np.cos(s)\n",
    "    init = np.array([r, c]).T \n",
    "\n",
    "# Display results of different maximum iterations to optimize snake shape\n",
    "i = 1\n",
    "plt.subplots(figsize=(12, 12)) \n",
    "for max_it in [10, 30, 60, 100]:\n",
    "    snake = active_contour(gaussian(img, sigma=3), #Smooth image. sigma is standard deviation for Gaussian kernel.\n",
    "                           init, #Initial snake coordinates\n",
    "                           alpha=0.015, #Snake length shape parameter. Higher values makes snake contract faster.\n",
    "                           beta=10, #Snake smoothness shape parameter. Higher values makes snake smoother.\n",
    "                           w_line=0, #Controls attraction to brightness.\n",
    "                           w_edge=1, #Controls attraction to edges. Use negative values to repel snake from edges.\n",
    "                           gamma=0.001, #Explicit time stepping parameter.\n",
    "                           max_iterations=max_it, #Maximum iterations toSmooth image before analyzing. sigma means standard deviation for Gaussian kernel.Smooth image before analyzing. sigma means standard deviation for Gaussian kernel. optimize snake shape.\n",
    "                           boundary_condition='periodic') # 'periodic' to fit a closed curve\n",
    "    plt.subplot(2,2,i) \n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if version < current:\n",
    "        plt.plot(init[:, 0], init[:, 1], '--r', lw=3) # initial circle - the dotted red circle\n",
    "        plt.plot(snake[:, 0], snake[:, 1], '-g', lw=3) # optimized snake shape - green lines\n",
    "    else:\n",
    "        plt.plot(init[:, 1], init[:, 0], '--r', lw=3) # initial circle - the dotted red circle\n",
    "        plt.plot(snake[:, 1], snake[:, 0], '-g', lw=3) # optimized snake shape - green lines\n",
    "    plt.title('max iteration=' + str(max_it))\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21c500",
   "metadata": {},
   "source": [
    "### 5.1 Exercise\n",
    "\n",
    "What happens as you increase or decrease the alpha and beta parameters? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d44e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff835687",
   "metadata": {},
   "source": [
    "## Bonus: Texture transfer with image quilting\n",
    "\n",
    "Rendering an object with a texture taken from a different object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57ddcc",
   "metadata": {},
   "source": [
    "### 6.1 Texture transfer example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a45f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images\n",
    "bill =  cv2.imread(os.path.join(rootpath, \"bill-big.jpg\"))\n",
    "rice = cv2.imread(os.path.join(rootpath, \"rice.png\"))\n",
    "\n",
    "bill_rgb = cv2.cvtColor(bill, cv2.COLOR_BGR2RGB)\n",
    "rice_rgb = cv2.cvtColor(rice, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the loaded images\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(bill_rgb)\n",
    "plt.title('Bill')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(rice_rgb)\n",
    "plt.title('Rice')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform texture transfer\n",
    "# Note: May take about 10-15 minutes to complete.\n",
    "res2 = myutils.transferIter(rice_rgb, bill_rgb, 20, 2)\n",
    "\n",
    "# Display the result of texture transfer\n",
    "plt.imshow(res2)\n",
    "plt.title('Mix')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb2f39",
   "metadata": {},
   "source": [
    "### 6.2 Texture transfer example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cce659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images\n",
    "man =  cv2.imread(os.path.join(rootpath, \"man.png\"))\n",
    "drawing = cv2.imread(os.path.join(rootpath, \"drawing.png\"))\n",
    "\n",
    "man_rgb = cv2.cvtColor(man, cv2.COLOR_BGR2RGB)\n",
    "drawing_rgb = cv2.cvtColor(drawing, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the loaded images\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(man_rgb)\n",
    "plt.title('Man')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(drawing_rgb)\n",
    "plt.title('Drawing')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform texture transfer\n",
    "# Note: May take about 10-15 minutes to complete.\n",
    "res3 = myutils.transfer(drawing_rgb, man_rgb, 20, blur=True)\n",
    "\n",
    "# Display the result of texture transfer\n",
    "plt.imshow(res3)\n",
    "plt.title('Mix')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e9319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
