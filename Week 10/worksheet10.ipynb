{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75182d2",
   "metadata": {},
   "source": [
    "# COMP90086 Workshop 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba31f9b",
   "metadata": {},
   "source": [
    "In this workshop, we will discuss generative models for images.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Autoencoders\n",
    "    - Basic Autoencoder\n",
    "    - Convolutional Autoencoder for Image Denoising\n",
    "    - (Extra bonus) Convolutional Variational Autoencoder \n",
    "- GANs\n",
    "    - Play with GANs in your browser\n",
    "    - DCGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28569c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a1947",
   "metadata": {},
   "source": [
    "# 1. Autoencoders\n",
    "\n",
    "Adapted from the Keras Blog and TensorFlow.org tutorials:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://blog.keras.io/building-autoencoders-in-keras.html\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/240px-Keras_logo.svg.png\" width=32/>\n",
    "    View source on blog.keras.io</a>\n",
    "  </td>   \n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/autoencoder\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View source on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5eca2",
   "metadata": {},
   "source": [
    "An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error. \n",
    "\n",
    "![autoencoder](https://blog.keras.io/img/ae/autoencoder_schema.jpg)\n",
    "<center>Image source: the Keras Blog</center>\n",
    "\n",
    "We will introduce autoencoders with three examples: basic autoencoder, image denoising, and convolutional variational autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9ca94",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset and Prepare the data\n",
    "We will train the basic autoencoder using the MNIST dataset. \n",
    "\n",
    "MNIST is a dataset that consists of images of handwritten digits:\n",
    "* the input data are images of handwritten digits (28×28 pixels with a single 8-bit channel)\n",
    "* the target is a label in the set $\\{0, 1, \\ldots, 9\\}$\n",
    "\n",
    "The data is already split into training and test sets. The training set contains 60,000 instances and the test set contains 10,000 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38df69",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits. We load the data into NumPy arrays using a built-in function from Keras, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only need images from the dataset to encode and decode,\n",
    "# we won't use the labels.\n",
    "(train_data, _), (test_data, test_labels) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the data\n",
    "train_data = train_data.astype('float32') / 255.\n",
    "test_data = test_data.astype('float32') / 255.\n",
    "\n",
    "x_train = train_data.reshape((len(train_data), np.prod(train_data.shape[1:])))\n",
    "x_test = test_data.reshape((len(test_data), np.prod(test_data.shape[1:])))\n",
    "\n",
    "print(\"train_images shape:\", x_train.shape)\n",
    "print(\"test_images shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22252990",
   "metadata": {},
   "source": [
    "## (1) Basic autoencoder\n",
    "\n",
    "Let's start simple, with a single fully-connected neural layer as encoder and as decoder.\n",
    "\n",
    "\n",
    "Define an autoencoder with two Dense layers: an `encoder`, which compresses the images into a 32 dimensional latent vector, and a `decoder`, that reconstructs the original image from the latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the size of our encoded representations\n",
    "latent_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(latent_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b864a",
   "metadata": {},
   "source": [
    "Let's also create a separate encoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd00a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d2f2d",
   "metadata": {},
   "source": [
    "As well as the decoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa382ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(latent_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a6be3",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "\n",
    "First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cd506",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f18f4c",
   "metadata": {},
   "source": [
    "After 20 epochs, the autoencoder seems to reach a stable train/validation loss value of about 0.09. We can try to visualize the reconstructed inputs and the encoded representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeeb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bfa9d",
   "metadata": {},
   "source": [
    "Define a `display` function to display ten random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be563ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 10 # How many digits we will display\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1.reshape(28, 28))\n",
    "        plt.title(\"original\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2.reshape(28, 28))\n",
    "        plt.gray()\n",
    "        plt.title(\"reconstructed\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the test data and the reconstructed results\n",
    "display(x_test, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d150815",
   "metadata": {},
   "source": [
    "Here's what we get. The top row is the original digits, and the bottom row is the reconstructed digits. The autoencoder is able to recreate digits quite accurately, although there is some information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b78d45",
   "metadata": {},
   "source": [
    "### Visualise the latent space\n",
    "\n",
    "The autoencoder has learned a latent representation of the images, which is a compressed representation (in this case, just 32 values) that represent the variation in the MNIST images. We can use [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualise the latent space. This is a form of dimensionality-reduction that tries to preserve the distances between points as much as possible -- points that are nearby in the latent space will be nearby in the 2D plot. Different digits are shown in different colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# using t-SNE to visualise the latent space \n",
    "tsne = TSNE(n_components=2) \n",
    "X_tsne = tsne.fit_transform(encoded_imgs) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c= test_labels, cmap='rainbow')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f5d2d",
   "metadata": {},
   "source": [
    "### Exercise 1: Investigate the latent space representation\n",
    "\n",
    "Try changing the size of the latent space. How does this affect the reconstruction results and the organisation of digits in the latent space? Why?\n",
    "\n",
    "You may notice the representations of different digits overlap in the latent space, particularly when the latent space is very small. Which digits cluster together in the latent space? Do these clusters make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741f417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52ab64f7",
   "metadata": {},
   "source": [
    "## (2) Convolutional autoencoder for image denoising\n",
    "\n",
    "An autoencoder can also be trained to remove noise from images. In the following section, you will create a noisy version of the MNIST dataset by applying random noise to each image. You will then train a deep convolutional autoencoder for image denoising, mapping noisy digits images from the MNIST dataset to clean digits images.\n",
    "\n",
    "Below we define a `noise` function to add random noise to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfe660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "\n",
    "    return np.clip(noisy_array, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81bdee0",
   "metadata": {},
   "source": [
    "Let's reuse the dataset we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba33e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "x_train = np.reshape(train_data, (len(train_data), 28, 28, 1))\n",
    "x_test = np.reshape(test_data, (len(test_data), 28, 28, 1))\n",
    "\n",
    "# Create a copy of the data with added noise\n",
    "noisy_train_data = noise(x_train)\n",
    "noisy_test_data = noise(x_test)\n",
    "\n",
    "# Display the train data and a version of it with added noise\n",
    "display(x_train, noisy_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d7a4c",
   "metadata": {},
   "source": [
    "Can our autoencoder learn to recover the original digits? Let's find out.\n",
    "\n",
    "**Build the autoencoder.**\n",
    "\n",
    "Compared to the previous convolutional autoencoder, in order to improve the quality of the reconstructed, we'll use a slightly different model with more filters per layer:\n",
    "\n",
    "We are going to use the Functional API to build our convolutional autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bff2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2f8af",
   "metadata": {},
   "source": [
    "Now we can train our autoencoder using the noisy data as our input and the clean data as our target. We want our autoencoder to learn how to denoise the images.\n",
    "\n",
    "**This may take a while, please refer to your tutor's demonstration of the results during the workshop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(\n",
    "    x=noisy_train_data,\n",
    "    y=x_train,\n",
    "    epochs=2,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(noisy_test_data, x_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f222e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(noisy_test_data)\n",
    "display(noisy_test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa8754",
   "metadata": {},
   "source": [
    "## (3) Extra bonus: Convolutional Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb54912",
   "metadata": {},
   "source": [
    "Variational autoencoders are a slightly more modern and interesting take on autoencoding.\n",
    "\n",
    "What is a variational autoencoder? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n",
    "\n",
    "Because a VAE is a more complex example, your could refer to the standalone script below:\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cvae\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2950e1",
   "metadata": {},
   "source": [
    "# 2. Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95febf4",
   "metadata": {},
   "source": [
    "## (1) Warm-up [Play with GANs in your browser!](https://poloclub.github.io/ganlab/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1626000",
   "metadata": {},
   "source": [
    "### GANs, Autoencoders and VAEs\n",
    "Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the “adversarial”) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation and voice generation.\n",
    "\n",
    "It may be useful to compare generative adversarial networks to other neural networks, such as autoencoders and variational autoencoders.\n",
    "\n",
    "Autoencoders encode input data as vectors. They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation.\n",
    "\n",
    "Variational autoencoders are generative algorithm that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN. However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred.\n",
    "\n",
    "(Source of the above introduction: [Pathmind](https://wiki.pathmind.com/generative-adversarial-network-gan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480bce4",
   "metadata": {},
   "source": [
    "## (2) DCGAN\n",
    "\n",
    "We will introduce how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/abs/1511.06434) (DCGAN). We will still use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\n",
    "\n",
    "Adapted from TensorFlow.org tutorials and Keras.io guides:\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/dcgan\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View source on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "   <td>\n",
    "    <a target=\"_blank\" href=\"https://keras.io/guides/writing_a_training_loop_from_scratch/\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Keras_logo.svg/240px-Keras_logo.svg.png\" width=32/>\n",
    "    View source on keras.io</a>\n",
    "  </td>  \n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2debc",
   "metadata": {},
   "source": [
    "### Load and prepare the dataset\n",
    "\n",
    "Let's reimport the dataset to omit the modifications made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = (x_test - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "print(\"train_images shape:\", x_train.shape)\n",
    "print(\"test_images shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863eb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd29ee2",
   "metadata": {},
   "source": [
    "### Create the models\n",
    "\n",
    "A GAN is made of two parts: a \"generator\" model that maps points in the latent space to points in image space, a \"discriminator\" model, a classifier that can tell the difference between real images (from the training dataset) and fake images (the output of the generator network).\n",
    "\n",
    "<img style=\"float: ;\" src=\"https://www.tensorflow.org/images/gan/dcgan.gif\" width=600 height=500>\n",
    "<center>(Image courtesy: tensorflow.org)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff15600",
   "metadata": {},
   "source": [
    "A GAN training loop looks like this:\n",
    "\n",
    "- (1) Train the generator. \n",
    "    - Sample random points in the latent space. \n",
    "    - Turn the points into fake images via the \"generator\" network. \n",
    "    - Get a batch of real images and combine them with the generated images. \n",
    "    - Train the \"generator\" model to \"fool\" the discriminator and classify the fake images as real.\n",
    "\n",
    "- (2) Train the discriminator. \n",
    "    - Sample a batch of random points in the latent space. \n",
    "    - Turn the points into fake images via the \"generator\" model. \n",
    "    - Get a batch of real images and combine them with the generated images. \n",
    "    - Train the \"discriminator\" model to classify generated vs. real images.\n",
    "\n",
    "Let's implement this training loop. \n",
    "\n",
    "#### The Generator\n",
    "\n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6b16d",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) generator to create an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dffb",
   "metadata": {},
   "source": [
    "#### The Discriminator\n",
    "Then, create the discriminator meant to classify fake vs real digits. The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93da9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f8712",
   "metadata": {},
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906513ec",
   "metadata": {},
   "source": [
    "#### Define the loss and optimizers\n",
    "Define loss functions and optimizers for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f8e65",
   "metadata": {},
   "source": [
    "Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679742",
   "metadata": {},
   "source": [
    "Generator loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e0da4",
   "metadata": {},
   "source": [
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae359198",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19de24",
   "metadata": {},
   "source": [
    "#### Save checkpoints\n",
    "\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07f249",
   "metadata": {},
   "source": [
    "#### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50 # In practice you need at least 20 epochs to generate nice digits.\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress after each epoch\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fde7cf",
   "metadata": {},
   "source": [
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11923258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for each epoch\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epochs,\n",
    "                             seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f3f1d",
   "metadata": {},
   "source": [
    "#### Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2946123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719addc5",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. \n",
    "\n",
    "To train your own model, run:\n",
    "\n",
    "`train(train_dataset, EPOCHS)`\n",
    "\n",
    "To restore the latest checkpoint with your own model, run:\n",
    "\n",
    "`checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))`\n",
    "\n",
    "To save time, we will use the prepared checkpoints directly in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15316f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Disable Tensorflow verbose logging output\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) # or any {DEBUG, INFO, WARNING, ERROR, CRITICAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Restore the pre-trained checkpoints\n",
    "# checkpoint.restore(\"./pretrained_checkpoints/ckpt-1\") #about 50 epochs\n",
    "checkpoint.restore(\"./pretrained_checkpoints/ckpt-2\") #about 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c03374",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = generator(seed, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1127a10",
   "metadata": {},
   "source": [
    "### Exercise 2: Check the Discriminator output for real and fake images\n",
    "\n",
    "Run some real images (from a test set) and fake images (N random samples from the Generator) through the Discriminator and check their scores. If the Discriminator is well-trained, it should output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab66c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 10 real images for test set and generate 10 fake images from the Generator\n",
    "real_images = x_test[:10]\n",
    "noise = tf.random.normal([10, noise_dim])\n",
    "fake_images = generator(noise, training=False)\n",
    "\n",
    "display(real_images, fake_images.numpy())\n",
    "\n",
    "# Run these real/fake images rough the Discriminator and check their scores.\n",
    "# your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458271a",
   "metadata": {},
   "source": [
    "The top row is the real images for the test set, and the bottom row is the fake images generated from the Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39633241",
   "metadata": {},
   "source": [
    "### Exercise 3: Check that generated images are not just copies of the training set\n",
    "\n",
    "Is the Generator simply repeating images from its training set? Or has it learned how to create new images?\n",
    "\n",
    "To check this, generate one random image from the Generator. Compute its similarity (e.g., sum of square pixel difference) to every image in the training set. Return the closest match (or 3 closest matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61bb44",
   "metadata": {},
   "source": [
    "### Exercise 4: Do a linear walk through the latent space\n",
    "In a well-trained generative model, the latent space should be **continuous** and **complete**. Continuous means that similar images are nearby in the latent space. Complete means that all points in the latent space correspond to real images.\n",
    "\n",
    "A good way to check this is to do a linear walk through the latent space. Select two random points (A,B) in the Generator's input space. Get N equally-spaced points from A to B (e.g., from `numpy.linspace`) and generate an image for each of those input points.\n",
    "\n",
    "If the GAN is well-trained, the images should show a smooth transition from A to B (indicating that the latent space is continuous) and every step between those points should look like a realistic image (indicating that the latent space is complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cb245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79a47f",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement [the birthday paradox test](https://arxiv.org/abs/1706.08224)\n",
    "\n",
    "Ideally, a generative model should be able to produce a wide variety of images. However, in practice GANs usually show some amount of \"mode collapse\" -- the Generator can only produce a limited number of images. In severe cases of mode collapse, a Generator will only learn to produce a single image.\n",
    "\n",
    "Less-severe cases of mode collapse are hard to detect, but one way to check for this is the [the birthday paradox test](https://arxiv.org/abs/1706.08224). Choose a value of N and generate N random images. See if you can spot any duplicate images in the set of N. The value of N at which you start seeing duplicates gives you an approximation of the number of images the GAN can produce. If you have a 50% chance of duplicates in a set of N, that suggests that the Generator produces approximately N^2 unique different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc0933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
