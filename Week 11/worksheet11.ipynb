{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T8YmTG4K5d1"
   },
   "source": [
    "# COMP90086 Workshop 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmhzR5g9K5d3"
   },
   "source": [
    "In this workshop, you will have some practice on image segmentation.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Pixel Clustering\n",
    "  - K-means\n",
    "  - Mean shift\n",
    "\n",
    "- Superpixel oversegmentation\n",
    "\n",
    "- Graph-based segmentation\n",
    "  - Region merging\n",
    "  - Normalized cuts\n",
    "\n",
    "- CNN-based segmentation\n",
    "  - Semantic segmentation\n",
    "  - Instance segmentation\n",
    "  \n",
    "- Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Anaconda user, \n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pixellib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74PSeIyxL_vp"
   },
   "outputs": [],
   "source": [
    "## For colab user, run\n",
    "\n",
    "#! pip install pixellib\n",
    "\n",
    "## After the update, You must restart the runtime in order to use newly installed versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6he1NotK5d3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Control the logging verbosity\n",
    "# Suppress verbose Tensorflow logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# Sets the AutoGraph verbosity level.\n",
    "tf.autograph.set_verbosity(0)\n",
    "# Verbosity is now 0\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.data import astronaut, coffee\n",
    "from skimage.color import rgb2gray, label2rgb\n",
    "from skimage.filters import sobel\n",
    "from skimage.future import graph\n",
    "from skimage.segmentation import felzenszwalb, slic, quickshift, watershed\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.cluster import spectral_clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Pixel Clustering\n",
    "\n",
    "This very simple approach to segmentation just involves clustering pixels into K clusters based on their RGB colour value.\n",
    "\n",
    "There are many different approaches in the field of image segmentation using pixel clustering, such as k-means, agglomerative, and mean-shift. We are going to discuss a popular method, the K-Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. A cluster refers to a collection of data points aggregated together because of certain similarities. The diagram below illustrates the process. \n",
    "\n",
    "<center>K-Means</center>\n",
    "<img style=\"float: ;\" src=\"https://miro.medium.com/max/1000/1*IzMqJo3MkZO6M38e709XqA.gif\" width=400 height=300>\n",
    "<center>(Image courtesy: Cierra Andaur)</center>\n",
    "\n",
    "You can also [interact with and visualise the process of K-Means Algorithm here](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/).\n",
    "\n",
    "For image segmentation, the clusters here are the different image colors. A practical application might be in the analysis of satellite images. For example, we might want to measure how much forest or desert there is in an area.\n",
    "\n",
    "To implement K-Means in Python, we use the built-in [KMeans( ) function from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and specify the number of clusters via the parameter `n_clusters = int (the number of clusters)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Load an image and check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image into BGR Format\n",
    "img =  cv2.imread('Albert_Park.jpg')\n",
    "# Convert to RGB\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(\"Image shape:\", img.shape)\n",
    "\n",
    "plt.imshow(img) # as RGB Format\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Convert a 3D image into a 2D matrix\n",
    "The kmeans( ) function takes a 2D array as input. Whereas our original image is 3D (width, height and channels), we need to flatten the height and width into a single vector of pixels (3 color values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image to a 2D array of pixels and 3 color values\n",
    "flat_image = img.reshape((-1,3))\n",
    "# Convert to float\n",
    "flat_image = np.float32(flat_image)\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "kmeans.fit(flat_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Integer format\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "print(centers) # In RGB Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot what colours these centres are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0,figsize=(8,2))\n",
    "\n",
    "i = 1\n",
    "for each_col in centers:\n",
    "    plt.subplot(1,4,i)\n",
    "    plt.axis(\"off\")\n",
    "    i+=1\n",
    "    \n",
    "    # Color Swatch\n",
    "    a = np.zeros((100,100,3),dtype='uint8')\n",
    "    a[:,:,:] = each_col\n",
    "    plt.imshow(a)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Segmentation of the original image (K=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = centers[kmeans.labels_]\n",
    "result_image1 = res.reshape((img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_image1)\n",
    "plt.title(\"Segmented Image when K = 4\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Segmentation of the original image with different K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use different K\n",
    "\n",
    "# image 2\n",
    "K = 10\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "kmeans.fit(flat_image)\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "res = centers[kmeans.labels_]\n",
    "result_image2 = res.reshape((img.shape))\n",
    "\n",
    "# image 3\n",
    "K = 2\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "kmeans.fit(flat_image)\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "res = centers[kmeans.labels_]\n",
    "result_image3 = res.reshape((img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them out\n",
    "figure_size = 8\n",
    "plt.figure(figsize=(figure_size,figure_size))\n",
    "#original image\n",
    "plt.subplot(2,2,1),plt.imshow(img)\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "#image 2\n",
    "plt.subplot(2,2,2),plt.imshow(result_image2)\n",
    "plt.title('Segmented Image when K = 10'), plt.xticks([]), plt.yticks([])\n",
    "#image 1\n",
    "plt.subplot(2,2,3),plt.imshow(result_image1)\n",
    "plt.title('Segmented Image when K = 4'), plt.xticks([]), plt.yticks([])\n",
    "#image 3\n",
    "plt.subplot(2,2,4),plt.imshow(result_image3)\n",
    "plt.title('Segmented Image when K = 2'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complicated image, the algorithm will need more clusters to pick out the details you want. The number of clusters will depend on how much detail you need to pick out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 \n",
    "Implement Pixel Clustering using the Mean-Shift Clustering Algorithm.\n",
    "\n",
    "[Mean-shift clustering](https://scikit-learn.org/stable/modules/clustering.html#mean-shift) aims to discover “blobs” in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n",
    "\n",
    "Different from the K-Means clustering algorithm, mean-shift does not require specifying  the number of clusters in advance. The number of clusters will be determined by algorithm for the data.\n",
    "\n",
    "It can be implemented via the built-in [MeanShift( ) function from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Read an image into BGR Format\n",
    "img =  cv2.imread('256kodim23.png')\n",
    "# Convert to RGB\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Reshape the image to a 2D array of pixels and 3 color values\n",
    "flat_image = img.reshape((-1,3))\n",
    "# Convert to float\n",
    "flat_image = np.float32(flat_image)\n",
    "\n",
    "# Get x,y coordinate values of pixels\n",
    "x, y = np.meshgrid(np.arange(img.shape[0]),np.arange(img.shape[1]))\n",
    "x = x.reshape((-1,1))\n",
    "y = y.reshape((-1,1))\n",
    "xy = np.concatenate((x,y),axis=1)\n",
    "\n",
    "# Since the sklearn function only allows one value for bandwidth, we will scale\n",
    "# the (x,y) values relative to the colour values. This has the same effect as\n",
    "# using different bandwidth values for space and colour.\n",
    "xy_scaling = 0.005\n",
    "xy = xy_scaling*xy\n",
    "\n",
    "# Combine colour and x,y values\n",
    "flat_image = np.concatenate((flat_image,xy),axis=1)\n",
    "\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering with MeanShift\n",
    "# your code\n",
    "\n",
    "# Segmentation of the original image\n",
    "# your code\n",
    "\n",
    "# Plot result\n",
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values for the xy_scaling and bandwidth parameters to see how it changes the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Superpixel-based Segmentation\n",
    "\n",
    "This approach is based on over-segmenting the image (usually by simple features like colour/edges) and then merging similar regions. \n",
    "\n",
    "\n",
    "Adapted from the [Docs for scikit-image](https://scikit-image.org/docs/dev/auto_examples/)\n",
    "\n",
    "### 2.1 Over-segmentation\n",
    "\n",
    "This demos compares four popular low-level image segmentation methods.  As\n",
    "it is difficult to obtain good segmentations, and the definition of \"good\"\n",
    "often depends on the application, these methods are usually used for obtaining\n",
    "an oversegmentation, also known as superpixels. \n",
    "\n",
    "#### Felzenszwalb's efficient graph based segmentation\n",
    "This fast 2D image segmentation algorithm, proposed in [[1]](http://cs.brown.edu/people/pfelzens/papers/seg-ijcv.pdf) is popular in the\n",
    "computer vision community.\n",
    "The algorithm has a single ``scale`` parameter that influences the segment\n",
    "size. The actual size and number of segments can vary greatly, depending on\n",
    "local contrast.\n",
    "\n",
    "\n",
    "#### Quickshift image segmentation\n",
    "\n",
    "Quickshift is a relatively recent 2D image segmentation algorithm, based on an\n",
    "approximation of kernelized mean-shift. Therefore it belongs to the family of\n",
    "local mode-seeking algorithms and is applied to the 5D space consisting of\n",
    "color information and image location [[2]](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi08quick.pdf).\n",
    "\n",
    "One of the benefits of quickshift is that it actually computes a\n",
    "hierarchical segmentation on multiple scales simultaneously.\n",
    "\n",
    "Quickshift has two main parameters: ``sigma`` controls the scale of the local\n",
    "density approximation, ``max_dist`` selects a level in the hierarchical\n",
    "segmentation that is produced. There is also a trade-off between distance in\n",
    "color-space and distance in image-space, given by ``ratio``.\n",
    "\n",
    "\n",
    "#### SLIC - K-Means based image segmentation\n",
    "\n",
    "This algorithm simply performs K-means in the 5d space of color information and\n",
    "image location and is therefore closely related to quickshift. As the\n",
    "clustering method is simpler, it is very efficient. It is essential for this\n",
    "algorithm to work in Lab color space to obtain good results.  The algorithm\n",
    "quickly gained momentum and is now widely used. See [[3]](https://core.ac.uk/download/pdf/147983593.pdf) for details.  The\n",
    "``compactness`` parameter trades off color-similarity and proximity, as in the\n",
    "case of Quickshift, while ``n_segments`` chooses the number of centers for\n",
    "kmeans.\n",
    "\n",
    "\n",
    "#### Compact watershed segmentation of gradient images\n",
    "\n",
    "Instead of taking a color image as input, watershed requires a grayscale\n",
    "*gradient* image, where bright pixels denote a boundary between regions.\n",
    "The algorithm views the image as a landscape, with bright pixels forming high\n",
    "peaks. This landscape is then flooded from the given *markers*, until separate\n",
    "flood basins meet at the peaks. Each distinct basin then forms a different\n",
    "image segment. [[4]](https://en.wikipedia.org/wiki/Watershed_%28image_processing%29)\n",
    "\n",
    "As with SLIC, there is an additional *compactness* argument that makes it\n",
    "harder for markers to flood faraway pixels. This makes the watershed regions\n",
    "more regularly shaped. [[5]](https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf)\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [img_as_float( )](https://scikit-image.org/docs/dev/api/skimage.html#skimage.img_as_float) Convert an image to floating point format.\n",
    "- [felzenszwalb( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb) Computes Felsenszwalb’s efficient graph based image segmentation.\n",
    "- [slic( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic) Segments image using k-means clustering in Color-(x,y,z) space.\n",
    "- [quickshift( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift) Segments image using quickshift clustering in Color-(x,y) space.\n",
    "- [watershed( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.watershed) Find watershed basins in image flooded from given markers.\n",
    "- [mark_boundaries( )](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.mark_boundaries) Return image with boundaries between labeled regions highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the astronaut image and convert the image to floating point format.\n",
    "img = img_as_float(astronaut()[::2, ::2])\n",
    "\n",
    "# Segments image using different methods\n",
    "segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)\n",
    "segments_slic = slic(img, n_segments=250, compactness=10, sigma=1,\n",
    "                     start_label=1)\n",
    "segments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)\n",
    "gradient = sobel(rgb2gray(img))\n",
    "segments_watershed = watershed(gradient, markers=250, compactness=0.001)\n",
    "\n",
    "# Count the number of segments\n",
    "print(f'Felzenszwalb number of segments: {len(np.unique(segments_fz))}')\n",
    "print(f'SLIC number of segments: {len(np.unique(segments_slic))}')\n",
    "print(f'Quickshift number of segments: {len(np.unique(segments_quick))}')\n",
    "print(f'Watershed number of segments: {len(np.unique(segments_watershed))}')\n",
    "\n",
    "# Plot them out\n",
    "figure_size = 8\n",
    "plt.figure(figsize=(figure_size,figure_size))\n",
    "# Felzenszwalbs's method\n",
    "# mark_boundaries allows us to see where we get the end of border of each segment\n",
    "plt.subplot(2,2,1),plt.imshow(mark_boundaries(img, segments_fz))\n",
    "plt.title(\"Felzenszwalbs's method\"), plt.axis('off')\n",
    "# SLIC\n",
    "plt.subplot(2,2,2),plt.imshow(mark_boundaries(img, segments_slic))\n",
    "plt.title('SLIC'), plt.axis('off')\n",
    "# Quickshift\n",
    "plt.subplot(2,2,3),plt.imshow(mark_boundaries(img, segments_quick))\n",
    "plt.title('Quickshift'), plt.axis('off')\n",
    "# Compact watershed\n",
    "plt.subplot(2,2,4),plt.imshow(mark_boundaries(img, segments_watershed))\n",
    "plt.title('Compact watershed'), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. [1] Efficient graph-based image segmentation, Felzenszwalb, P.F. and\n",
    "       Huttenlocher, D.P.  International Journal of Computer Vision, 2004\n",
    "       \n",
    ".. [2] Quick shift and kernel methods for mode seeking,\n",
    "       Vedaldi, A. and Soatto, S.\n",
    "       European Conference on Computer Vision, 2008\n",
    "       \n",
    ".. [3] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi,\n",
    "    Pascal Fua, and Sabine Suesstrunk, SLIC Superpixels Compared to\n",
    "    State-of-the-art Superpixel Methods, TPAMI, May 2012.\n",
    "    \n",
    ".. [4] https://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n",
    "\n",
    ".. [5] Peer Neubert & Peter Protzel (2014). Compact Watershed and\n",
    "       Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation\n",
    "       Algorithms. ICPR 2014, pp 996-1001. :DOI:`10.1109/ICPR.2014.181`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Graph-based Segmentation\n",
    "\n",
    "Many segmentation approaches treat the image pixels (or superpixels) as a graph and find an optimal segmentation by connecting or disconnecting nodes in the graph.\n",
    "\n",
    "### 3.1 Region merging\n",
    "\n",
    "In this section, we will discuss how to utilise the Region Adjacency Graph (RAG) for combining over-segmented regions of an image to obtain better segmentation. The following demo constructs a RAG and progressively merges regions that are similar in color. Merging two adjacent regions produces a new region with all the pixels from the merged regions. Regions are merged until no highly similar region pairs remain.\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [graph.rag_mean_color( )](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.rag_mean_color) Compute the Region Adjacency Graph using mean colors.\n",
    "- [graph.merge_hierarchical( )](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.merge_hierarchical) Perform hierarchical merging of a RAG.\n",
    "- [label2rgb( )](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb) Return an RGB image where color-coded labels are painted over the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_mean_color(g, src, dst, n):\n",
    "    \"\"\"Callback to handle merging nodes by recomputing mean color.\n",
    "\n",
    "    The method expects that the mean color of `dst` is already computed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    g : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    n : int\n",
    "        A neighbor of `src` or `dst` or both.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary with the `\"weight\"` attribute set as the absolute\n",
    "        difference of the mean color between node `dst` and `n`.\n",
    "    \"\"\"\n",
    "\n",
    "    diff = g.nodes[dst]['mean color'] - g.nodes[n]['mean color']\n",
    "    diff = np.linalg.norm(diff)\n",
    "    return {'weight': diff}\n",
    "\n",
    "\n",
    "def merge_mean_color(g, src, dst):\n",
    "    \"\"\"Callback called before merging two nodes of a mean color distance graph.\n",
    "\n",
    "    This method computes the mean color of `dst`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    \"\"\"\n",
    "    g.nodes[dst]['total color'] += g.nodes[src]['total color']\n",
    "    g.nodes[dst]['pixel count'] += g.nodes[src]['pixel count']\n",
    "    g.nodes[dst]['mean color'] = (g.nodes[dst]['total color'] /\n",
    "                                      g.nodes[dst]['pixel count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Coffee cup image\n",
    "img = coffee() \n",
    "\n",
    "# Use the SLIC algorithm to segment the input image and obtain the region labels\n",
    "labels = slic(img, compactness=30, n_segments=400, start_label=1)\n",
    "# Compute the Region Adjacency Graph using mean colors.\n",
    "g = graph.rag_mean_color(img, labels)\n",
    "out = label2rgb(labels, img, kind='avg', bg_label=0)\n",
    "out = np.uint8(out)\n",
    "\n",
    "# Perform hierarchical merging of a RAG.\n",
    "# Greedily merges the most similar pair of nodes until no edges lower than 'thresh' remain.\n",
    "labels2 = graph.merge_hierarchical(labels, g, \n",
    "                                   thresh=35, #Regions connected by an edge with weight smaller than thresh are merged.\n",
    "                                   rag_copy=False,\n",
    "                                   in_place_merge=True,\n",
    "                                   merge_func=merge_mean_color,#This function is called before merging two nodes.\n",
    "                                   weight_func=_weight_mean_color)#The function to compute the new weights of the nodes adjacent to the merged node.\n",
    "\n",
    "# Return an RGB image where color-coded labels are painted over the image.\n",
    "out2 = label2rgb(labels2, img, kind='avg', bg_label=0)\n",
    "out2 = np.uint8(out2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(6, 8))\n",
    "\n",
    "ax[0].imshow(out)\n",
    "ax[1].imshow(out2)\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalized cuts\n",
    "\n",
    "Instead of merging regions, let's try separating regions using normalized cuts. As in the previous example, we first oversegment the image into superpixels and connect them into a Region Adjacency Graph (RAG). Then we cut the graph into regions using Normalized Cuts.\n",
    "\n",
    "More information on the functions used below, see:\n",
    "- [graph.cut_normalized( )](https://scikit-image.org/docs/dev/api/skimage.future.graph.html#skimage.future.graph.cut_normalized) Perform Normalized Graph cut on the Region Adjacency Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = coffee()\n",
    "\n",
    "labels = slic(img, compactness=30, n_segments=400,\n",
    "                            start_label=1)\n",
    "out = label2rgb(labels, img, kind='avg', bg_label=0)\n",
    "\n",
    "g = graph.rag_mean_color(img, labels, mode='similarity')\n",
    "labels_nc = graph.cut_normalized(labels, g)\n",
    "out_nc = label2rgb(labels_nc, img, kind='avg', bg_label=0)\n",
    "\n",
    "out = np.uint8(out)\n",
    "out_nc = np.uint8(out_nc)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(6, 8))\n",
    "\n",
    "ax[0].imshow(out)\n",
    "ax[1].imshow(out_nc)\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Try different values for the threshold (thresh) in the hierarchical merge function. What is the effect of changing the merging threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 CNN-based segmentation\n",
    "\n",
    "In this section, we will use PixelLib to perform semantic segmentation and instance segmentation with pretrained model.\n",
    "\n",
    "[Pixellib](https://github.com/ayoolaolafenwa/PixelLib) is a library for performing segmentation of objects in images and videos. It supports the two major types of image segmentation:\n",
    "\n",
    "    1.Semantic segmentation\n",
    "\n",
    "    2.Instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixellib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Semantic segmentation\n",
    "\n",
    "PixelLib is implemented with Deeplabv3+ framework to perform semantic segmentation. In this demo we load [the Xception model trained](https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5) on [PASCAL VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) for Semantic Segmentation. This dataset has 20 object categories.\n",
    "\n",
    "<center>Objects categories and their corresponding color maps of PASCAL VOC dataset</center>\n",
    "<img style=\"float: ;\" src=\"https://raw.githubusercontent.com/ayoolaolafenwa/PixelLib/e6af2be69d12d06e307670ba55eda309ca463a8c/Tutorials/Images/pascal.png\" width=300 height=400>\n",
    "<center>(Image courtesy: PixelLib)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixellib.semantic import semantic_segmentation\n",
    "\n",
    "# Created an instance of semantic segmentation class\n",
    "segment_image = semantic_segmentation()\n",
    "\n",
    "# Call the function to load the xception model trained on pascal voc.\n",
    "segment_image.load_pascalvoc_model(\"deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\") \n",
    "\n",
    "# Performs segmentation on an image and the segmentation is done in the pascalvoc's color format.\n",
    "segment_image.segmentAsPascalvoc(\"sample4.jpg\", output_image_name = \"output.jpg\")\n",
    "# Obtain an image with segmentation overlay on the objects by setting overlay = True\n",
    "segment_image.segmentAsPascalvoc(\"sample4.jpg\", output_image_name = \"overlap.jpg\", overlay = True)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "# Read an image into BGR Format and Convert to RGB\n",
    "img = cv2.cvtColor(cv2.imread('sample4.jpg'),cv2.COLOR_BGR2RGB)\n",
    "img1 = cv2.cvtColor(cv2.imread('output.jpg'),cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('overlap.jpg'),cv2.COLOR_BGR2RGB)\n",
    "# Plot them out\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(3,1,1),plt.imshow(img)\n",
    "plt.title(\"Original Image\"), plt.axis('off')\n",
    "plt.subplot(3,1,2),plt.imshow(img1)\n",
    "plt.title('Semantic Segmentation Result'), plt.axis('off')\n",
    "plt.subplot(3,1,3),plt.imshow(img2)\n",
    "plt.title('Image with Segmentation Overlay'), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Instance segmentation\n",
    "\n",
    "Instance segmentation with PixelLib is based on MaskRCNN framework.\n",
    "\n",
    "In this demo we load [the mask rcnn model trained](https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.2/mask_rcnn_coco.h5) on the [COCO dataset](https://cocodataset.org/#home) to perform instance segmentation. COCO is a dataset with 80 common object categories. The model can perform instance segmentation on these object categories. You can explore the COCO dataset by using the [COCO dataset explorer page](https://cocodataset.org/#explore).\n",
    "\n",
    "\n",
    "A list of the object categories present in Coco dataset:\n",
    "\n",
    "[‘BG’, ‘person’, ‘bicycle’, ‘car’, ‘motorcycle’, ‘airplane’, ‘bus’, ‘train’, ‘truck’, ‘boat’, ‘traffic light’, ‘fire hydrant’, ‘stop sign’, ‘parking meter’, ‘bench’, ‘bird’, ‘cat’, ‘dog’, ‘horse’, ‘sheep’, ‘cow’, ‘elephant’, ‘bear’, ‘zebra’, ‘giraffe’, ‘backpack’, ‘umbrella’, ‘handbag’, ‘tie’, ‘suitcase’, ‘frisbee’, ‘skis’, ‘snowboard’, ‘sports ball’, ‘kite’, ‘baseball bat’, ‘baseball glove’, ‘skateboard’, ‘surfboard’, ‘tennis racket’, ‘bottle’, ‘wine glass’, ‘cup’, ‘fork’, ‘knife’, ‘spoon’, ‘bowl’, ‘banana’, ‘apple’, ‘sandwich’, ‘orange’, ‘broccoli’, ‘carrot’, ‘hot dog’, ‘pizza’, ‘donut’, ‘cake’, ‘chair’, ‘couch’, ‘potted plant’, ‘bed’, ‘dining table’, ‘toilet’, ‘tv’, ‘laptop’, ‘mouse’, ‘remote’, ‘keyboard’, ‘cell phone’, ‘microwave’, ‘oven’, ‘toaster’, ‘sink’, ‘refrigerator’, ‘book’, ‘clock’, ‘vase’, ‘scissors’, ‘teddy bear’, ‘hair drier’, ‘toothbrush’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixellib.instance import instance_segmentation\n",
    "\n",
    "# Created an instance of instance segmentation class\n",
    "segment_image = instance_segmentation()\n",
    "\n",
    "# Load the mask rcnn model to perform instance segmentation.\n",
    "segment_image.load_model(\"mask_rcnn_coco.h5\") \n",
    "\n",
    "# Perform instance segmentation on an image\n",
    "segment_image.segmentImage(\"sample4.jpg\", output_image_name = \"img_new.jpg\")\n",
    "# Implement segmentation with bounding boxes.\n",
    "segment_image.segmentImage(\"sample4.jpg\", output_image_name = \"img_boxes.jpg\", show_bboxes = True)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "# Read an image into BGR Format and Convert to RGB\n",
    "img = cv2.cvtColor(cv2.imread('sample4.jpg'),cv2.COLOR_BGR2RGB)\n",
    "img1 = cv2.cvtColor(cv2.imread('img_new.jpg'),cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('img_boxes.jpg'),cv2.COLOR_BGR2RGB)\n",
    "# Plot them out\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(3,1,1),plt.imshow(img)\n",
    "plt.title(\"Original Image\"), plt.axis('off')\n",
    "plt.subplot(3,1,2),plt.imshow(img1)\n",
    "plt.title('Instance Segmentation with Overlay'), plt.axis('off')\n",
    "plt.subplot(3,1,3),plt.imshow(img2)\n",
    "plt.title('Instance Segmentation with Bounding Boxes'), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained coco model used detects 80 classes of objects. PixelLib has made it possible to filter out unused detections and detect the classes you want.\n",
    "\n",
    "We use the function select_target_classes that determines the target class to be detected. In this case we want to detect only person in the image. In the function segmentImage we added a new parameter segment_target_classes to filter unused detections and detect only the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of Target Classes\n",
    "seg = instance_segmentation()\n",
    "seg.load_model(\"mask_rcnn_coco.h5\")\n",
    "target_classes = seg.select_target_classes(person=True)\n",
    "seg.segmentImage(\"sample4.jpg\", segment_target_classes= target_classes, show_bboxes=True, output_image_name=\"target.jpg\")\n",
    "\n",
    "# Display the results\n",
    "# Read an image into BGR Format and Convert to RGB\n",
    "img = cv2.cvtColor(cv2.imread('sample4.jpg'),cv2.COLOR_BGR2RGB)\n",
    "img1 = cv2.cvtColor(cv2.imread('target.jpg'),cv2.COLOR_BGR2RGB)\n",
    "# Plot them out\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1),plt.imshow(img)\n",
    "plt.title(\"Original Image\"), plt.axis('off')\n",
    "plt.subplot(1,2,2),plt.imshow(img1)\n",
    "plt.title('Detection of Target Classes'), plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Try using other sample images and see the results of semantic segmentation/instance segmentation. Try to find cases where the model fails. Why do you think these cases are difficult for the classification-based approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about PixelLib, please refer to [PIXELLIB'S OFFICIAL DOCUMENTATION](https://pixellib.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "You can also implement custom training on your own dataset using PixelLib’s Library by reading this [tutorial](https://pixellib.readthedocs.io/en/latest/custom_train.html).\n",
    "Or learn how how to perform inference with your custom model with PixelLib by reading this [tutorial](https://pixellib.readthedocs.io/en/latest/custom_inference.html#inference-with-a-custom-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "worksheet07_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
